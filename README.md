
# ğŸ§  Research Assistant with LLM + Chroma Vector Store

This project is designed to assist **researchers or document readers** in **answering questions based on the context of uploaded PDF documents**. By leveraging language models and vector databases, the assistant can understand, store, and retrieve information from your local research documents.

---

## ğŸ“‚ Project Structure

```
research-assistant/
â”‚
â”œâ”€â”€ main.py                        # Main script to run the pipeline
â”œâ”€â”€ .env                           # Environment variables (not pushed to Git)
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ research_assistant.ipynb       # Jupyter notebook version of the pipeline
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.py                  # Stores constants like API keys, paths, model names
â”‚
â”œâ”€â”€ embeddings/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ embedder.py                # Loads the embedding model
â”‚
â”œâ”€â”€ loaders/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ md_loader.py               # Loads and chunks markdown/text files
â”‚
â”œâ”€â”€ vectorstore/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ store.py                   # Create/load Chroma vectorstore
â”‚
â””â”€â”€ qa_chain/
    â”œâ”€â”€ __init__.py
    â””â”€â”€ chain.py                   # Creates a RetrievalQA chain using the retriever and LLM
```

---

## ğŸš€ Features

- âœ… Load and chunk local markdown/text documents
- âœ… Generate custom embeddings using OpenAI-compatible APIs
- âœ… Store and retrieve documents from **Chroma vector store**
- âœ… Ask questions using RetrievalQA with source documents
- âœ… Modular design for maintainability and reuse
- âœ… Jupyter Notebook included for exploration

---

## ğŸ§ª Getting Started

### 1. Clone the Repository

```bash
git clone https://github.com/your-username/research-assistant.git
cd research-assistant
```

### 2. Create `.env` file

Create a `.env` file in the root directory with the following variables:

```
API_BASE=https://your-llm-api-base-url
API_KEY=your_api_key
MODEL_NAME=your_embedding_model_name
CHROMA_DB_PATH=./chroma_db
DOCS_PATH=./documents
```

> âœ… Make sure `.env` is listed in `.gitignore` so your keys are not exposed.

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

---

## ğŸ§  Usage

### ğŸŸ¢ Option 1: Run from Python script

```bash
python main.py
```

### ğŸŸ¡ Option 2: Explore via Jupyter Notebook

Open `research_assistant.ipynb` in Jupyter or VS Code and run the cells step-by-step.

---

## ğŸ” Example Query

```python
query = "What is the function of vitamin B12 in the human body?"
response = qa_chain.run(query)
print(response)
```

---

## ğŸ“˜ Dependencies

- `langchain`
- `langchain-openai`
- `langchain-community`
- `chromadb`
- `python-dotenv`

---

## ğŸ›¡ï¸ Security Notes

- Keep your `.env` file private.
- Never commit API keys or sensitive paths to GitHub.

---

## ğŸ“š Credits

This project uses:
- [LangChain](https://www.langchain.com/)
- [Chroma](https://www.trychroma.com/)
- OpenAI-compatible LLM endpoints

---

## ğŸ“Œ To Do

- [ ] Add support for PDFs (currently markdown/text support)
- [ ] Add semantic accuracy evaluation
- [ ] Streamlit web interface

---

## ğŸ“„ License

MIT License
