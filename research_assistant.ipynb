{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cae19b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install markitdown\n",
    "# !pip install PyMuPDF\n",
    "# !pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "34f12600",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b559bdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access variables\n",
    "api_key = os.getenv(\"LLM_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e01170",
   "metadata": {},
   "source": [
    "### Read PDF files and convert them in markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e5e6a917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9a2e95da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and output folder paths\n",
    "input_folder = \"/Users/xitij/Documents/RAG_Projects/Papers/PDF\"\n",
    "output_folder = \"/Users/xitij/Documents/RAG_Projects/Papers/MD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "27fbf988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "db2767c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: paper4.pdf → paper4.md\n",
      "Processed: PD_paper_175.pdf → PD_paper_175.md\n",
      "Processed: 2111.09741v1.pdf → 2111.09741v1.md\n"
     ]
    }
   ],
   "source": [
    "# Process all PDF files in the input folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.lower().endswith(\".pdf\"):\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "        output_filename = os.path.splitext(filename)[0] + \".md\"\n",
    "        output_path = os.path.join(output_folder, output_filename)\n",
    "\n",
    "        # Read PDF content\n",
    "        doc = fitz.open(input_path)\n",
    "        full_text = \"\"\n",
    "        for page in doc:\n",
    "            full_text += page.get_text() + \"\\n\\n\"\n",
    "        doc.close()\n",
    "\n",
    "        # Save as markdown file\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(full_text)\n",
    "\n",
    "        print(f\"Processed: {filename} → {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7bcd62",
   "metadata": {},
   "source": [
    "### Loading the md files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "03ccebd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "89f235f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder where .md files are saved\n",
    "markdown_folder = \"/Users/xitij/Documents/RAG_Projects/Papers/MD\"\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    path=markdown_folder, \n",
    "    glob=\"**/*.md\",                  # Load all .md files (supports recursive search)\n",
    "    loader_cls=TextLoader,           # Use TextLoader to load each file\n",
    "    use_multithreading=True          # Speeds up loading for large folders\n",
    ")\n",
    "\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e5c7a705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/Users/xitij/Documents/RAG_Projects/Papers/MD/PD_paper_175.md'}, page_content='On the Role of Preprocessing on Matching Tables to\\nKnowledge Graphs\\nVishvapalsinhji Parmar1, Achraf Haddar1 and Alsayed Algergawy1\\n1Chair of Data and Knowledge Engineering, University of Passau Passau, Germany\\nAbstract\\nMatching tabular data to knowledge graphs plays a crucial role in various applications, including Named Entity\\nRecognition (NER). Data preprocessing has consistently shown to enhance the performance of data-driven systems.\\nTo this end, in this paper, we present a systematic preprocessing pipeline designed to improve the accuracy\\nof table-to-graph matching by identifying and addressing anomalies in datasets from diverse domains, such as\\nbiodiversity, food, and Wikidata. Our pipeline, implemented over three iterations, focuses on correcting domain-\\nspecific irregularities to enhance data quality. Experimental results demonstrate substantial improvements, with\\nF1 score increases of up to 50% in the food domain and 5% in biodiversity, surpassing existing methods. These\\nadvancements contribute to more efficient data interpretation and analysis across a variety of sectors.\\nKeywords\\nTable Annotation, Table Understanding, Preprocessing\\n1. Introduction\\nThe exponential growth of online information offers substantial opportunities across various domains.\\nHowever, the data is often in diverse and fragmented formats, categorized into structured, semi-\\nstructured, and unstructured forms. Among these, tabular data is widely used due to its readability\\nand compactness, serving applications in fields such as medicine, climate change, biodiversity, and\\nmanufacturing [1]. Despite its prevalence, extracting meaningful information from tabular data remains\\nchallenging due to limited context, necessitating alignment with semantic artifacts like ontologies and\\nknowledge graphs [2]. There are also some Knowledge graphs, which are networks of interconnected\\nentities and concepts, provide a robust foundation for advanced data interpretation by leveraging\\nrelational information to uncover insights, support decision-making, and enhance analytics [3]. Align-\\ning tabular data with knowledge graphs is thus a promising avenue for innovation. The Semantic\\nTable Understanding (SemTab) Challenge has fostered advancements in algorithms that link tabular\\ndata to knowledge graphs, enhancing data comprehension1. A study in this field demonstrated that\\nleveraging preprocessed cells using the Being Search API2 can enhance system performance [4]. While\\nmany systems addressing this challenge focus on candidate generation, table element processing, and\\ndisambiguation [2], the diverse potential of the preprocessing stage remains underexplored.\\nTo bridge this gap, we introduce a preprocessing pipeline designed to enhance data quality for table-\\nto-knowledge graph matching. We carried out a number of experiments using datasets from Wikidata,\\nBiodiversity, and Food Tables from the SemTab challenge. The results show notable improvements in\\nCell Entity Annotation (CEA) performance. The implementation code and notebooks for reproducibility\\nare available in our repository3.\\nEKAW 2024: EKAW 2024 Workshops, Tutorials, Posters and Demos, 24th International Conference on Knowledge Engineering and\\nKnowledge Management (EKAW 2024), November 26-28, 2024, Amsterdam, The Netherlands.\\nEnvelope-Open vishvapalsinhji.parmar@uni-passau.de (V. Parmar); alsayed.algergawy@uni-passau.de (A. Algergawy)\\nOrcid 0000-0002-4370-2729 (V. Parmar); 0000-0002-8550-4720 (A. Algergawy)\\n© 2024 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).\\n1https://www.cs.ox.ac.uk/isg/challenges/sem-tab/\\n2https://www.microsoft.com/en-us/bing/apis/bing-web-search-api\\n3https://github.com/DKEPassau/PreprocessMatch\\nCEUR\\nWorkshop\\nProceedings\\nceur-ws.org\\nISSN 1613-0073\\n\\n\\n2. Motivation\\nData preprocessing is a critical stage in matching tables to knowledge graphs, yet it is often over-\\nlooked. Although tabular data is inherently structured, it frequently contains noise, missing values,\\nand inconsistencies that impede accurate interpretation. Effective preprocessing—comprising data\\ncleaning, normalization, feature extraction, and transformation—enhances data quality, ensuring that\\nit is ready for further processing and matching tasks. Our aim is to explore how preprocessing can\\nsignificantly impact the performance of aligning tables to knowledge graphs. This study investigates\\nvarious preprocessing techniques, such as handling missing values, language inconsistencies, and\\nspecial characters, to improve data quality. We also examine the effectiveness of these techniques in the\\ncontext of the Semantic Table Understanding (SemTab) challenge, where the primary goal is to achieve\\nbetter annotation accuracy through systematic preprocessing.\\nBy developing and implementing a structured preprocessing pipeline, we address specific challenges\\nassociated with different datasets. This study demonstrates how targeted preprocessing can enhance\\ntable annotation results, reduce errors, and ultimately improve the integration of tabular data with\\nknowledge graphs, paving the way for more accurate and insightful data analysis.\\n3. Dataset and Exploratory Data Analysis (EDA)\\nTo evaluate the effectiveness of our preprocessing pipeline, we employed three distinct datasets from\\nthe SemTab challenge: Wikidata Tables, BioDiversity Tables, and tFood Tables. Each dataset presents\\nunique characteristics and challenges, requiring a tailored approach to preprocessing. Conducting a\\nthorough Exploratory Data Analysis (EDA) was a critical first step in understanding these datasets,\\nidentifying their specific structures, and determining the necessary preprocessing steps to improve\\nannotation accuracy.\\n3.1. Wikidata Tables\\nThe Wikidata Tables dataset consists of synthetic data generated from the Wikidata knowledge graph\\nusing SPARQL queries. This dataset spans a wide range of domains and contains 500 tables, each stored\\nas a separate CSV file. Each table includes a primary column designated as the subject, with additional\\ncolumns providing contextual information. EDA revealed several data quality issues, including:\\n• Missing Values: Approximately 43.6% of the tables contain missing values, with 751 instances\\nidentified across the dataset. Since these values were not annotated in the original data, no\\nimputation was performed.\\n• Language Inconsistencies: About 32.28% of the cell values are in non-English languages,\\nidentified using the ’Detect Language API.’ This tool was chosen over the ’langid’ Python package\\ndue to its superior accuracy in identifying languages like English, French, German, Spanish,\\nItalian, and Arabic.\\n• Special Characters and Misspellings: Various cells contain special characters or spelling errors\\n(e.g., ’City of Porsmouth’ instead of ’City of Portsmouth’), which could hinder the retrieval of\\ncorrect annotations. These issues were systematically addressed through data cleaning rules in\\nthe preprocessing pipeline.\\n3.2. BioDiversity Tables\\nThe BioDiversity Tables dataset comprises 50 tables derived from real-world biodiversity research and\\nmanually annotated samples. It leverages three public repositories: data.world, BEFChina, and BExIS,\\nand is characterized by four unique features: Specimen data, Numerical data, Abbreviations, and Special\\nformats. EDA findings for this dataset include:\\n\\n\\n• Numerical and String Data Types: Over 54% of the columns contain numerical data, while 33%\\nare strings. Most tables (49 out of 50) feature at least four columns, except for one single-column\\ntable.\\n• Missing Values: Similar to Wikidata, this dataset shows a high number of missing values\\n(39,198), which also lack corresponding annotations in the CEA target file, indicating no need for\\nimputation.\\n• Domain-Specific Patterns: The dataset contains domain-specific characteristics, such as species\\nname abbreviations (e.g., ’C. sclerophylla’ for ’Castanopsis sclerophylla’) and composed values\\ncombining multiple elements. Tools like the NCBI Taxonomy database and ChatGPT were used\\nto interpret these domain-specific nuances, ensuring accurate data preprocessing.\\n3.3. tFood Tables\\nThe tFood dataset is designed specifically for the Food domain, including two types of tables: Horizontal\\nRelational Tables and Entity Tables. Each table contains two columns, with one column representing\\nentity properties (e.g., Prop0, Prop1) and the other providing detailed descriptions. Key findings from\\nthe EDA include:\\n• String Data Type: All cells earmarked for annotation are of the string type, requiring consistent\\nhandling of language and special characters.\\n• Language Detection: Using the ’Detect Language API,’ it was identified that 11.92% of the\\nannotated cells contain non-English content, necessitating appropriate preprocessing rules to\\nmanage multilingual data.\\n• Composed Values and Special Cases: Some columns feature values composed of multiple\\nelements separated by characters like hyphens (’-’). Identifying these patterns was crucial to\\ndeveloping targeted preprocessing rules to ensure proper annotation.\\n3.4. Common Anomalies\\nIn the analysis of tabular datasets, such as those from Wikidata, BioDiversity, and tFood, several common\\nanomalies were identified that can hinder the accuracy of matching tables to knowledge graphs. These\\nanomalies include:\\n• Missing Values: Frequently occurring in datasets, missing values can lead to incomplete or\\ninaccurate data interpretation. For example, in the Wikidata and BioDiversity tables, a significant\\npercentage of tables exhibited missing values, necessitating careful handling to avoid bias or\\nerrors in subsequent annotations.\\n• Language Inconsistencies: Datasets often contain content in multiple languages, complicating\\nthe retrieval of correct entities. For instance, over 32% of the Wikidata tables and nearly 12% of\\nthe tFood tables included non-English content. The use of the ’Detect Language API’ helps in\\nidentifying and standardizing these inconsistencies.\\n• Special Characters and Misspellings: The presence of special characters (e.g., punctuation\\nmarks or symbols) and misspellings can mislead APIs or annotation tools, resulting in empty or\\nincorrect query results. Correcting such errors is critical for improving data accuracy.\\n• Cell Duplications and Composed Values: Duplicated data entries or values composed of multi-\\nple elements (e.g., ’North-Lincolnshire’) can create ambiguities in data interpretation. Developing\\nrules to handle these cases ensures more accurate alignment of tabular data with knowledge\\ngraphs.\\n• Abbreviations and Special Cases: Domain-specific abbreviations (e.g., ’C. sclerophylla’) require\\ncontext-aware processing to ensure accurate annotation. Failure to correctly interpret these cases\\nmay result in significant annotation errors.\\n\\n\\nAddressing these anomalies is essential for maintaining the integrity of the data annotation process.\\nTo ensure a consistent and effective approach to preprocessing, we developed a systematic pipeline for\\nanomaly detection and resolution. This pipeline is designed to apply standardized preprocessing steps\\nacross different datasets, reducing human intervention and enhancing overall annotation accuracy.\\n4. Pipeline\\nTo effectively address the identified anomalies, we propose a robust pipeline for preprocessing and\\nannotating cell values in tabular datasets. The pipeline aims to enhance the efficiency and accuracy of\\ntable annotation tasks by systematically addressing data quality issues. The proposed pipeline unfolds\\nin three main phases, each targeting specific types of abnormalities identified during EDA. Figure 1\\nprovides an overview of the sequential steps involved in the pipeline.\\nFigure 1: Proposed pipeline for table annotation\\nPhase 1: Detection of Obvious Anomalies : The first phase focuses on detecting and resolving\\nobvious abnormalities in the dataset, such as special characters, missing values, and composed values\\nseparated by delimiters (e.g., hyphens). By applying specific preprocessing rules, we aim to standardize\\nthese elements to reduce ambiguity and improve the quality of the data being annotated.\\nPhase 2: Identification of Domain-Related Anomalies : The second phase addresses more com-\\nplex anomalies, such as language inconsistencies, misspellings, and domain-specific abbreviations. This\\ninvolves using tools like the ’Detect Language API’ to identify non-English content and implementing\\ntargeted rules to correct misspellings and interpret abbreviations accurately. The goal is to reduce the\\nnumber of incorrect or empty annotations.\\nPhase 3: Refinement and Advanced Preprocessing : The final phase involves refining the\\nannotations further by exploring special cases and developing additional preprocessing rules to handle\\ncomplex data patterns. This phase includes a comprehensive analysis of special characters, abbreviations,\\nand composed values that could not be fully resolved in the earlier phases. The focus is on achieving\\nthe highest possible annotation accuracy by applying a refined set of rules tailored to the unique\\ncharacteristics of each dataset.\\nAnnotation Retrieval Process : After preprocessing, the refined dataset is ready for the annotation\\nretrieval process. This involves querying external knowledge graphs to retrieve accurate annotations\\nfor each cell value. The process leverages contextual information from rows to select the correct entity\\nfrom potential matches, ensuring a high degree of precision in the final annotations.\\n\\n\\n5. Experiments and Future Direction\\nTo evaluate the performance of our preprocessing pipeline, we conducted experiments on three datasets:\\nWikidata, BioDiversity, and tFood. Each dataset underwent the proposed preprocessing steps, which\\nwere designed to address specific data anomalies such as missing values, language inconsistencies,\\nspecial characters, and domain-specific patterns. The primary objective was to assess the impact of\\npreprocessing on the Cell Entity Annotation (CEA) task accuracy. The F1 score was used as the main\\nevaluation metric, representing the harmonic mean of Precision and Recall, to measure the effectiveness\\nof our pipeline. Significant improvements were observed across all datasets, validating the robustness of\\nour approach. The results showed an increase in F1 scores of 5% up to 50% in some cases, demonstrating\\nthe pipeline’s effectiveness in enhancing annotation accuracy. Table 1 presents a summary of the F1\\nscores before and after applying the preprocessing pipeline, highlighting the improvements achieved.\\nTable 1\\nF1 score before and after applying pipeline\\nDataset\\nF1 Score\\nPrevious System Score\\nBefore\\nAfter\\nF1 Score\\nWikidata Tables\\n0.681\\n0.845\\n0.88\\nBioDiversity Tables\\n0.385\\n0.696\\n0.64\\ntFood Tables\\n0.821\\n0.858\\n0.234\\nThe experimental results confirmed that targeted preprocessing steps, such as language detection,\\nhandling of special characters, and domain-specific adjustments, can significantly improve F1 scores.\\nOur approach reduces errors and increases the reliability of data integration with knowledge graphs,\\nproviding a strong foundation for further development in this field. Future work may explore the\\noptimization and automation of preprocessing pipelines and their application to additional domains.\\nIntegrating advanced machine learning techniques could further enhance anomaly detection and\\nresolution, unlocking new possibilities for extracting insights from tabular data and advancing Semantic\\nTable Understanding. Previous work, such as DREIFLUSS [5], highlights the potential to leverage\\npreprocessed data, which can ultimately enhance system performance.\\nReferences\\n[1] R. Shwartz-Ziv, A. Armon, Tabular data: Deep learning is not all you need, Information Fusion 81\\n(2022) 84–90.\\n[2] J. Liu, Y. Chabot, R. Troncy, V. Huynh, T. Labbé, P. Monnin, From tabular data to knowledge graphs:\\nA survey of semantic table interpretation tasks and methods, J. Web Semant. 76 (2023) 100761.\\ndoi:10.1016/j.websem.2022.100761.\\n[3] A. Hogan, E. Blomqvist, M. Cochez, C. D’amato, G. D. Melo, C. Gutierrez, S. Kirrane, J. E. L. Gayo,\\nR. Navigli, S. Neumaier, A.-C. N. Ngomo, A. Polleres, S. M. Rashid, A. Rula, L. Schmelzeisen,\\nJ. Sequeda, S. Staab, A. Zimmermann, Knowledge graphs, ACM Comput. Surv. 54 (2021). URL:\\nhttps://doi.org/10.1145/3447772. doi:10.1145/3447772.\\n[4] E. G. Henriksen, A. M. Khorsid, E. Nielsen, A. M. Stück, A. S. Sørensen, O. Pelgrin, Semtex: A hybrid\\napproach for semantic table interpretation, in: Proceedings of the Semantic Web Challenge on\\nTabular Data to Knowledge Graph Matching, SemTab 2023, ISWC 2023, Athens, Greece, November\\n6-10, 2023, volume 3557 of CEUR Workshop Proceedings, CEUR-WS.org, 2023, pp. 38–49. URL:\\nhttps://ceur-ws.org/Vol-3557/paper3.pdf.\\n[5] V. R. Parmar, A. Algergawy, DREIFLUSS: A minimalist approach for table matching, in: Proceedings\\nof the Semantic Web Challenge on Tabular Data to Knowledge Graph Matching, SemTab 2023,\\nISWC 2023, Athens, Greece, November 6-10, 2023, volume 3557 of CEUR Workshop Proceedings,\\nCEUR-WS.org, 2023, pp. 50–60. URL: https://ceur-ws.org/Vol-3557/paper4.pdf.\\n\\n\\n'),\n",
       " Document(metadata={'source': '/Users/xitij/Documents/RAG_Projects/Papers/MD/paper4.md'}, page_content='DREIFLUSS: A Minimalist Approach for Table\\nMatching\\nVishvapalsinhji Parmar1, Alsayed Algergawy1\\n1Chair of Data and Knowledge Engineering, University of Passau Passau, Germany\\nAbstract\\nThis paper introduces DREIFLUSS, an innovative, minimalist approach designed to tackle the Column\\nType Annotation (CTA) and Column Property Annotation (CPA) tasks in the SemTab challenge. DREI-\\nFLUSS efficiently employs semantic information from well-established knowledge graphs, DBpedia,\\nand Schema.org, to improve the annotation process. Experimental evidence illustrates the superior\\nperformance of logistic regression models trained via DREIFLUSS, resulting in precise column-type\\nannotations and insightful relationship predictions. The findings substantiate the significance of proper\\nsampling technique while training a model, thereby boosting the accuracy and efficiency of table match-\\ning. This research illuminates a promising pathway to enhance table matching techniques, underlining\\nthe practical ramifications of DREIFLUSS for data integration and knowledge discovery endeavors.\\nKeywords\\nTable matching, Column Type Annotation (CTA), Column Property Annotation (CPA), knowledge\\ndiscovery, data integration\\n1. Introduction\\nTable matching, as a critical part of data integration and knowledge discovery, is gaining\\nincreasing attention in this age of digital information proliferation. It harmonizes information\\nacross different tables, thereby paving the way for extracting valuable insights. An estimated\\nmillions of high-quality tables are currently accessible on the Internet, a figure that continues to\\nrise due to the progression of automated data extraction techniques and an increasing reliance\\non structured data across various sectors, including business, academia, and government [1].\\nIn this context, the SemTab challenge1 emerges as a pivotal competition, advancing the\\nfrontiers of table understanding and annotation. The challenge emphasizes the importance of\\nColumn Type Annotation (CTA) and Column Property Annotation (CPA) tasks, accentuating the\\nsignificance of accurate column labeling and the identification of inter-column relationships for\\ncomprehensive table understanding. Addressing this challenge, we introduce “DREIFLUSS”, a\\nminimalist yet effective approach tailored for the tasks presented in the SemTab challenge. DREI-\\nFLUSS harnesses the labels defined in two major knowledge graphs, DBpedia and Schema.org,\\nas a guiding force to improve the table matching process. The labels in these knowledge graphs\\nSemTab’23: Semantic Web Challenge on Tabular Data to Knowledge Graph Matching 2023, co-located with the 22nd\\nInternational Semantic Web Conference (ISWC), November 6-10, 2023, Athens, Greece\\nEnvelope-Open vishvapalsinhji.parmar@uni-passau.de (V. Parmar); alsayed.algergawy@uni-passau.de (A. Algergawy)\\nOrcid 0000-0002-4370-2729 (V. Parmar); 0000-0002-8550-4720 (A. Algergawy)\\n© 2023 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).\\nCEUR\\nWorkshop\\nProceedings\\nhttp://ceur-ws.org\\nISSN 1613-0073\\nCEUR Workshop Proceedings (CEUR-WS.org)\\n1https://www.cs.ox.ac.uk/isg/challenges/sem-tab/\\n\\n\\nprovide an exhaustive set of semantic details that can improve the accuracy and efficiency of\\ntable matching.\\nDBpedia, a vast knowledge graph sourced from Wikipedia, presents a broad spectrum of\\nlabels spanning various domains. Conversely, Schema.org, a collaborative initiative by leading\\nsearch engines, offers a universally accepted schema vocabulary. By leveraging these labels,\\nDREIFLUSS underscores the potential of using pre-existing semantic resources to enhance the\\ntable annotation process. This strategic utilization of label resources simplifies the implementa-\\ntion, while enhancing the system’s scalability and adaptability - key traits in the current era of\\ndata explosion [2, 3]. The ensuing sections offer a comprehensive overview of the CTA and\\nCPA tasks, data specifics, the experimental design we used, and the outcomes achieved. The\\ndetailed evaluation of the DREIFLUSS system clarifies its capabilities in handling table matching\\ntasks within the SemTab challenge and beyond.\\nThe rapid proliferation of structured data on the web opens up vast opportunities for knowl-\\nedge discovery and data integration but also introduces significant challenges. The data is\\ntypically embedded in tables, each with its own unique structure, schema, and notation. Extract-\\ning valuable and accessible information from these tables calls for advanced methods of data\\nunderstanding and harmonization. In this context, competitions such as SemTab take center\\nstage, pushing the boundaries of table understanding and annotation. Of the tasks involved,\\nColumn Type Annotation (CTA) and Column Property Annotation (CPA) are pivotal. These\\ntasks entail precise labeling of columns and establishing relationships among them, both crucial\\nfor comprehensive table understanding, efficient data integration, and adequate knowledge\\ndiscovery.\\nTo address these needs, we present an innovative methodology explicitly designed to tackle\\nthese tasks. While minimalist in its approach, this system leverages the existing labels in\\ntwo prominent knowledge graphs - DBpedia and Schema.org. These labels represent a rich,\\ncomprehensive, and standardized set of semantic details that can considerably enhance the\\nprecision and efficiency of the table matching process. Our solution aims to respond effectively\\nto the SemTab challenge and underlines the value of utilizing pre-existing semantic resources\\nto enhance table annotation. In a time when the need for scalable, adaptable solutions is more\\npressing than ever, our methodology stands out for its potential to cater to the growing needs\\nof the digital era.\\n2. Related Work\\nThe SemTab challenge, which began in 2019, has played a pivotal role in pushing the boundaries\\nof semantic table interpretation, a field that aims to understand and annotate tabular data with\\nsemantic information. In its inaugural year, 2019, the challenge witnessed some groundbreaking\\ncontributions. Oliveira and d’Aquin introduced “ADOG” [4], a system that leverages ontologies\\nfor annotating data. This was complemented by the work of Cremaschi et al., who presented\\n“MantisTable” [5], an innovative system designed to automatically interpret tables semantically.\\nAnother notable contribution was from Thawani et al., who delved deep into the CTA and\\nCPA tasks, presenting a method for linking entities to knowledge graphs, thereby inferring\\ncolumn types and properties [6]. The subsequent year, 2020, saw the challenge grow in terms\\n\\n\\nof participation and complexity. Huynh et al. unveiled an enhanced version of “DAGOBAH”\\n[7], which emphasized the importance of scalable annotations for large datasets. Parallelly,\\nAbdelmageed and Schindler introduced “JenTab” [8], a system tailored to match tabular data\\nwith knowledge graphs, bridging the gap between structured and unstructured data. By 2021,\\nthe challenge had gained significant traction in the research community. Systems that had\\nmade their debut in previous years underwent refinements. For instance, “DAGOBAH” [9]\\nwas further optimized to provide efficient semantic annotations. Similarly, “MantisTable V”\\n[10] was introduced as a novel and efficient successor to the earlier version, emphasizing\\ninnovative methods for table interpretation. The 2022 edition of the challenge was particularly\\nnoteworthy for the introduction of specialized datasets. “SOTAB” [11] and “MammoTab” [12]\\nwere introduced, both of which align closely with the 2023 round 2 tasks focusing on Schema.org\\nannotations. Additionally, systems like “s-elBat” by Cremaschi et al. [13] underscored the\\nchallenges and intricacies of interpreting real-world, messy data. As we approach the 2023\\nSemTab challenge, the emphasis on CTA and CPA tasks, especially in the context of Schema.org\\nand DBpedia, is more pronounced than ever. The collective works from 2019 to 2022 not only\\nhighlight the progress made but also set the stage for future innovations in the domain.\\n3. Tasks\\nThe second round of the SemTab challenge spotlights two core tasks: Column Type Annotation\\n(CTA) and Column Property Annotation (CPA). These tasks seek to enrich table comprehension\\nby attributing specific labels to columns and establishing inter-column relationships, respec-\\ntively.\\n3.1. Column Type Annotation (CTA)\\nCTA categorizes columns by associating specific labels that signify semantic information about\\ntheir content. This involves attributing fitting labels to columns based on their purpose or\\ncontent. In the context of the SemTab challenge, labels used for CTA derive from the DBpedia\\nand Schema.org knowledge graphs. CTA aids efficient data integration and allows downstream\\napplications to understand the structure and semantics of tables, proving pivotal in tasks such as\\ndata cleaning, schema matching, and query optimization. The labels assigned to column types\\noffer valuable insights into each column’s intended purpose and content, facilitating improved\\ndata understanding and analysis.\\n3.2. Column Property Annotation (CPA)\\nCPA focuses on establishing relationships between table columns. It involves annotating column\\npairs with labels indicating their mutual relationship or connection. These relationships could\\ndenote concepts such as “startDate,” “priceValidUntil,” or “recipeIngredient,” among others. CPA\\naffords essential context about inter-column relationships, leading to a more comprehensive\\nunderstanding of the table. The annotations help identify related or interconnected columns,\\nwhich is beneficial in data integration, schema alignment, and knowledge discovery tasks. By\\n\\n\\nTable 1\\nTrain CSV file for CTA\\ntable_name\\ncolumn_index\\nlabel\\n0\\nBook_11x17.pt\\n_September2020_CTA.json.gz\\n3\\nhttps://dbpedia.org/ontology/date\\n1\\nBook_12min.com\\n_September2020_CTA.json.gz\\n0\\nhttps://dbpedia.org/ontology/Book\\n2\\nBook_12min.com\\n_September2020_CTA.json.gz\\n2\\nhttps://dbpedia.org/ontology/Language\\nTable 2\\nExample of content in table\\n0\\n1\\n2\\n3\\n0\\n9789722539739\\nA Cidade Perdida\\n728\\n2020-07-10\\n1\\n9789722527118\\nMisery\\n480\\n2013-09-13\\ncapturing the relationships between columns, CPA bolsters the potential to extract meaningful\\ninsights from tables and supports precise analysis and decision-making.\\nCTA and CPA tasks collectively aim to improve table matching and comprehension. These\\ntasks amplify tables’ semantic richness by attributing labels to column types and defining\\ninter-column relationships, facilitating effective data integration, knowledge discovery, and\\nother downstream applications. The following sections will delve into the datasets used for CTA\\nand CPA, elucidate the experimental setup, discuss the results achieved using the DREIFLUSS\\nsystem, and evaluate our approach’s effectiveness in addressing these tasks in the SemTab\\nchallenge.\\n4. Dataset\\nThe SemTab 2023 competition page2 provided the datasets used for the CTA and CPA tasks. Each\\ntask has three dataset folders. For CTA, these folders are Round2-SOTAB-CTA-Tables, Round2-\\nSOTAB-CTA-DBP-Datasets, and Round2-SOTAB-CTA-SCH-Datasets. The Round2-SOTAB-\\nCTA-Tables folder contains 44,409 JSON files representing different tables, each file comprising\\nthe column index and corresponding value. The remaining two folders, Round2-SOTAB-CTA-\\nDBP-Datasets, and Round2-SOTAB-CTA-SCH-Datasets contain the training, validation, and test\\ndatasets in CSV format, along with the appropriate labels (derived from DBpedia or Schema.org)\\nin a TXT file. This file encompasses 46 labels for DBpedia and 80 labels for Schema.org. The\\nCSV files present the data in the format shown in Table 1. The validation dataset shares the\\nsame format, whereas the test dataset does not contain labels. The JSON files in the Tables\\nfolder can be converted to a table format which can be represented as shown in Table 2\\nThe data folders are similar in structure for the CPA task, encompassing 49 labels for DBpedia\\n2https://sem-tab-challenge.github.io/2023/\\n\\n\\nTable 3\\nTrain CSV file for CPA\\ntable_name\\nmain_column_index\\ncolumn_index\\nlabel\\n0\\nBook_11x17.pt\\n_September2020_CPA.json.gz\\n0\\n3\\ndatePublished\\n1\\nBook_1jour-1jeu.com\\n_September2020_CPA.json.gz\\n0\\n5\\ndatePublished\\nTable 4\\nCTA table after concatenating respective data value column\\ntable_name\\ncolumn_index\\nlabel\\ndata_value\\n0\\nBook_11x17.pt\\n_September2020_CTA.json.gz\\n3\\nDate\\n[2020-07-10, 2016-04-08,\\n2013-09-13, 2016-08-0...\\n1\\nBook_12min.com\\n_September2020_CTA.json.gz\\n0\\nBook/name\\n[The Sleep Revolution,\\nViva, Ame, Lidere, The ...\\nand 105 for Schema.org. The Tables folder contains 28,223 JSON files (tables), and the CSV data\\nrepresentation for this task includes an additional column to identify the primary column index,\\nas shown in Table 3. As with the CTA task, the test dataset for the CPA task does not include\\nlabels.\\nThe comprehensive dataset provided in the SemTab 2023 competition enhances the complexity\\nand richness of the CTA and CPA tasks, laying the groundwork for evaluating and refining the\\nefficacy of different table matching strategies.\\n5. Methodology\\nIn order to appraise the efficacy of the DREIFLUSS system, we employed a series of tests using\\ndatasets made available by the SemTab challenge. These datasets were specifically designed for\\nColumn Type Annotation (CTA) and Column Property Annotation (CPA) tasks. Our feature\\ngeneration process involved parsing the provided CSV file to extract relevant data points. The\\nfeatures were derived from the semantic information of the columns, the relationships between\\ncolumns, and the inherent structure of the tables. Initially, when attempting to process the\\nentire dataset on our local machine, we faced computational challenges due to the vastness of\\nthe data. To address this, we strategically downsampled the data. We ensured our sample set\\nencompassed representations from each category for both the CTA and CPA tasks. Specifically,\\nwe selected 15 samples for each label in the CTA task and 3 samples for each label in the\\nCPA task. This approach allowed us to maintain a balanced representation of each label while\\noptimizing computational efficiency.\\nNext, we extracted appropriate data values for the respective column indexes from the JSON\\nfile in the table dataset folder. This approach allowed us to construct new tables specifically for\\nthe CTA and CPA tasks, respectively. For the CTA task, we introduced a single column named\\n“data_value”, as illustrated in Table 4, designed to hold specific data values associated with each\\n\\n\\nTable 5\\nCPA table after concatenating respective main column and other column\\ntable_name\\nmain_\\ncolumn_\\nindex\\ncolumn_\\nindex\\nlabel\\nmain_\\ncolumn_\\nvalue\\nother_\\ncolumn_\\nvalue\\n0\\nMovie_yts-torrent.net\\n_September2020_\\nCPA.json.gz\\n0\\n9\\nactor\\n[The Lost\\nHusband\\n(2020),\\nJohn Henry\\n(2020)...\\n[[Nora Dunn,\\nKevin\\nAlejandro,\\n...\\n1\\nMovie_zomraa.com\\n_September2020_\\nCPA.json.gz\\n0\\n9\\nactor\\n[Integrity\\n(2019),\\nGretel &\\nHansel\\n(2020)...\\n[Johnny\\nDepp,\\nJohnny\\nDepp,\\n...\\nFigure 1: Experiment pipeline\\ncolumn type. In contrast, for the CPA task, we integrated two columns: “main_column_value”\\nand “other_column_value”, as showcased in Table 5. These columns captured the primary data\\nvalues and their associated or related values, respectively. Given the nature of our data, these\\ncolumns often contained lists of multiple elements. To streamline our data for modeling, we\\nemployed a technique known as “exploding”, which transformed each list of values into separate\\nrows. For instance, a row with a list of three elements in the “data_value” column would post-\\nexploding result in three distinct rows, each holding one of those elements. This transformation\\nensured a singular data point representation for each row, facilitating the subsequent training\\nprocess.\\nIn our experimental setup, we strategically selected columns from the generated tables to\\nserve as features for our machine-learning tasks. For the Column Type Annotation (CTA)\\ntask, our primary features were derived from the label and data_value columns. In contrast,\\nfor the Column Property Annotation (CPA) task, we harnessed the information from the\\nmain_column_value, other_column_value, and label columns.\\n\\n\\nBefore feeding this data into our machine learning model, we partitioned it into training\\nand testing sets, maintaining an 80-20 split. This division was done with stratification on the\\nlabel column, ensuring that our test set was representative of the overall distribution of labels.\\nThe next crucial step was data vectorization. Raw textual data isn’t directly usable in most\\nmachine learning algorithms, including logistic regression. Hence, we employed a vectorization\\ntechnique, specifically the CountVectorizer from scikit-learn, which converts text data into a\\nmatrix of token counts. This transformation is pivotal as it translates our textual data into a\\nnumerical format that our model can understand and learn from. With our data appropriately\\nvectorized, we proceeded to train our logistic regression model. We utilized scikit-learn’s\\nLogisticRegression class. By default, this model applies L2 regularization (ridge regularization)\\nwith a penalty hyperparameter set to ‘l2’. The strength of this regularization is controlled by\\nthe ‘C’ hyperparameter, which defaults to 1.0, implying a balanced regularization. The solver\\nhyperparameter, set to ‘lbfgs’, dictates the optimization algorithm used for parameter tuning.\\nAdditionally, the model iterates a maximum of 100 times during training, as determined by the\\n‘max_iter’ hyperparameter. Post-training, our logistic regression model had learned the intricate\\nmappings between our input features (vectorized data values) and the target outputs, which\\nwere either column type labels (for CTA) or column relationship labels (for CPA). To gauge the\\nmodel’s efficacy, we employed evaluation metrics like precision, recall, and F1 scores. These\\nmetrics provided insights into how well our model could predict column types and discern\\ninter-column relationships. A comprehensive visual representation of our entire experimental\\npipeline is depicted in Figure 1. For the broader research community’s benefit and to promote\\nreproducibility, we’ve made our implementation code publicly accessible on GitHub3. Through\\nthis rigorous methodology, we were able to critically assess the capabilities of the DREIFLUSS\\nsystem in the context of the SemTab challenge.”\\n6. Results\\nThis section delineates the findings obtained from the experiments carried out utilizing our\\nsystem for the Column Type Annotation (CTA) and Column Property Annotation (CPA) tasks.\\nThe results pertain to the use of both Schema.org and DBpedia labels. The efficacy of the logistic\\nregression models was gauged using evaluation metrics such as precision, recall, and F1 scores.\\nThe metrics illustrated in Table 1 present the performance of the models for both CTA and CPA\\ntasks using Schema.org and DBpedia labels. For the CTA task, the metrics indicate the model’s\\neffectiveness in precisely predicting column type labels based on the data values provided. In\\nthe context of the CPA task, these scores underscore the model’s proficiency in identifying\\nrelationships between columns based on the main and other column values.\\nThese results accentuate the system’s competence in carrying out both the CTA and CPA\\ntasks using Schema.org and DBpedia labels. The precision, recall, and F1 scores achieved\\nexemplify the system’s ability to predict column types and delineate relationships between\\ncolumns accurately. These outcomes pave the way for enhanced table matching, enabling\\ncomprehensive data integration and promoting knowledge discovery. Table 6 displays the\\nprecision, recall, and F1 scores for the CTA and CPA tasks using both Schema.org and DBpedia\\n3https://github.com/vishvapalsinh/cta-cpa-schemaorg-dbpedia\\n\\n\\nTable 6\\nPrecision, recall, and F1 scores for CTA and CPA tasks with Schema.org and DBpedia labels.\\nTask\\nLabels\\nOur Test Data\\nSemTab Test Data\\nPrecision\\nRecall\\nF1 Score\\nPrecision\\nF1 Score\\nCTA\\nSchema.org\\n89.47%\\n73.50%\\n77.95%\\n56.67%\\n38.04%\\nCTA\\nDBpedia\\n89.55%\\n70.87%\\n76.94%\\n61.14%\\n40.97%\\nCPA\\nSchema.org\\n91.77%\\n72.35%\\n78.01%\\n31.96%\\n17.39%\\nCPA\\nDBpedia\\n92.94%\\n77.73%\\n83.12%\\n39.67%\\n20.69%\\nlabels. It also shows the precision and F1 score generated on the test dataset provided by the\\nSemTab organizers4. The data demonstrates the system’s competence in executing these tasks\\neffectively.\\n7. Discussion\\nThe experimental outcomes from the DREIFLUSS system application for the Column Type\\nAnnotation (CTA) and Column Property Annotation (CPA) tasks during the SemTab challenge\\nform an insightful basis for subsequent discussion. This segment dives deeper into the reper-\\ncussions of the achieved results and initiates a dialogue around various facets linked with the\\nsystem’s performance, limitations, and potential enhancements.\\n7.1. Performance Analysis\\nThe precision, recall, and F1 scores achieved underscore the robustness of the logistic regression\\nmodels that DREIFLUSS employs for CTA and CPA tasks with both Schema.org and DBpedia\\nlabels. Although we used only a fraction of the available samples for training (CTA: 690-1200\\nout of 85561-115562, CPA: 147-315 out of 62128-97967), the model still yielded convincing results.\\nHowever, a decrease in scores on the SemTab-2023 challenge test set highlights the need for\\nmore effective sample selection strategies.\\n7.2. Significance of Knowledge Graphs\\nThe DREIFLUSS system benefits significantly from labels provided by Schema.org and DBpedia\\nknowledge graphs, reaffirming the importance of integrating existing semantic resources in\\ntable matching tasks. The broad coverage of DBpedia and the standardized schema vocabulary\\nof Schema.org serve as a rich data source, enriching the annotation process and enhancing\\nunderstanding of column types and relationships.\\n7.3. Limitations and Challenges\\nDespite showing promising outcomes, the DREIFLUSS system does have certain limitations. Its\\nheavy reliance on the quality and completeness of labels offered by DBpedia and Schema.org,\\n4https://shorturl.at/kFKO9\\n\\n\\nand the data values used for training can pose a challenge. Only complete or accurate labels\\nor data values can impact the system’s performance, resulting in incorrect classifications or\\nrelationship annotations. Furthermore, employing a logistic regression model may limit the\\nsystem’s ability to handle complex relationships or certain data variations.\\n7.4. Future Directions\\nFuture works could incorporate advanced techniques such as deep learning models or ensemble\\nmethods to overcome these limitations and augment the results. External knowledge sources\\nbeyond DBpedia and Schema.org, like domain-specific ontologies or other domain-specific\\nknowledge graphs, can offer more precise annotations. Additionally, considering more context,\\nsuch as table structure or content, could further enhance the accuracy of column type annotations\\nand relationship predictions.\\n7.5. Practical Applications\\nThe DREIFLUSS system holds practical significance in numerous domains and applications\\nrevolving around table understanding and integration. Its precise column type annotations and\\nrelationship predictions can aid in tasks such as data cleaning, schema matching, query opti-\\nmization, and knowledge discovery. Its minimalist design and reliance on existing knowledge\\ngraphs promise practicality and scalability in real-world applications.\\n8. Conclusion\\nThis study sheds light on the novel application of the DREIFLUSS system for the Column Type\\nAnnotation (CTA) and Column Property Annotation (CPA) tasks as a part of the SemTab chal-\\nlenge. The results obtained through the application of this system underscore its effectiveness\\nin enhancing table matching accuracy and efficiency. Through the use of Schema.org and DB-\\npedia labels, this research highlights the importance of integrating existing semantic resources\\ninto the process of table understanding. These knowledge graphs serve as invaluable tools,\\nproviding a rich data source that guides the annotation process and enhances the understanding\\nof column types and relationships. However, the journey towards a perfect solution is paved\\nwith challenges. Certain limitations, such as the dependence on the quality and completeness\\nof labels and data values used for training, have been identified. The use of logistic regression\\nmodels may also restrict the system’s ability to capture complex relationships or handle data\\nvariations. Nevertheless, these challenges present avenues for future research. Prospective\\nadvancements in this field could explore the inclusion of more sophisticated techniques such as\\ndeep learning models or ensemble methods. Expanding the scope to include domain-specific on-\\ntologies or other domain-specific knowledge graphs could provide more precise and specialized\\nannotations. The implications of this research are vast and multi-faceted. Beyond the academic\\nrealm, it holds substantial practical value in various domains and applications, such as data\\ncleaning, schema matching, query optimization, and knowledge discovery. The scalability and\\npracticality of the DREIFLUSS system promise its relevance in real-world scenarios.\\n\\n\\nIn conclusion, the DREIFLUSS system has demonstrated promising results in addressing\\nthe CTA and CPA tasks within the SemTab challenge, setting a solid foundation for further\\nimprovements. The learnings from this research open up exciting possibilities for future\\nendeavors in the realm of table matching and understanding, thereby contributing to the body\\nof knowledge in this ever-evolving field.\\nReferences\\n[1] A. O. Shigarov, Table understanding: Problem overview, WIREs Data Mining Knowl.\\nDiscov. 13 (2023). URL: https://doi.org/10.1002/widm.1482.\\n[2] C. Bizer, J. Lehmann, G. Kobilarov, S. Auer, C. Becker, R. Cyganiak, S. Hellmann, Dbpedia\\n- A crystallization point for the web of data, J. Web Semant. 7 (2009) 154–165. URL:\\nhttps://doi.org/10.1016/j.websem.2009.07.002.\\n[3] R. V. Guha, D. Brickley, S. Macbeth, Schema.org: Evolution of structured data on the web,\\nACM Queue 13 (2015) 10. URL: https://doi.org/10.1145/2857274.2857276.\\n[4] D. Oliveira, M. d’Aquin, ADOG - annotating data with ontologies and graphs, volume\\n2553 of CEUR Workshop Proceedings, CEUR-WS.org, 2019, pp. 1–6. URL: https://ceur-ws.\\norg/Vol-2553/paper1.pdf.\\n[5] M. Cremaschi, R. Avogadro, D. Chieregato, Mantistable: an automatic approach for the\\nsemantic table interpretation, volume 2553 of CEUR Workshop Proceedings, CEUR-WS.org,\\n2019, pp. 15–24. URL: https://ceur-ws.org/Vol-2553/paper3.pdf.\\n[6] A. Thawani, M. Hu, E. Hu, H. Zafar, N. T. Divvala, A. Singh, E. Qasemi, P. A. Szekely,\\nJ. Pujara, Entity linking to knowledge graphs to infer column types and properties,\\nvolume 2553 of CEUR Workshop Proceedings, CEUR-WS.org, 2019, pp. 25–32. URL: https:\\n//ceur-ws.org/Vol-2553/paper4.pdf.\\n[7] V. Huynh, J. Liu, Y. Chabot, T. Labbé, P. Monnin, R. Troncy, DAGOBAH: enhanced scoring\\nalgorithms for scalable annotations of tabular data, volume 2775 of CEUR Workshop\\nProceedings, CEUR-WS.org, 2020, pp. 27–39. URL: https://ceur-ws.org/Vol-2775/paper3.pdf.\\n[8] N. Abdelmageed, S. Schindler, Jentab: Matching tabular data to knowledge graphs, volume\\n2775 of CEUR Workshop Proceedings, CEUR-WS.org, 2020, pp. 40–49. URL: https://ceur-ws.\\norg/Vol-2775/paper4.pdf.\\n[9] V. Huynh, J. Liu, Y. Chabot, F. Deuzé, T. Labbé, P. Monnin, R. Troncy, DAGOBAH: table\\nand graph contexts for efficient semantic annotation of tabular data, volume 3103 of CEUR\\nWorkshop Proceedings, CEUR-WS.org, 2021, pp. 19–31. URL: https://ceur-ws.org/Vol-3103/\\npaper2.pdf.\\n[10] R. Avogadro, M. Cremaschi, Mantistable V: A novel and efficient approach to semantic\\ntable interpretation, volume 3103 of CEUR Workshop Proceedings, CEUR-WS.org, 2021, pp.\\n79–91. URL: https://ceur-ws.org/Vol-3103/paper7.pdf.\\n[11] K. Korini, R. Peeters, C. Bizer, SOTAB: the WDC schema.org table annotation benchmark,\\nvolume 3320 of CEUR Workshop Proceedings, CEUR-WS.org, 2022, pp. 14–19. URL: https:\\n//ceur-ws.org/Vol-3320/paper1.pdf.\\n[12] M. Marzocchi, M. Cremaschi, R. Pozzi, R. Avogadro, M. Palmonari, Mammotab: A giant and\\n\\n\\ncomprehensive dataset for semantic table interpretation, volume 3320 of CEUR Workshop\\nProceedings, CEUR-WS.org, 2022, pp. 28–33. URL: https://ceur-ws.org/Vol-3320/paper3.pdf.\\n[13] M. Cremaschi, R. Avogadro, D. Chieregato, s-elbat: A semantic interpretation approach\\nfor messy table-s, volume 3320 of CEUR Workshop Proceedings, CEUR-WS.org, 2022, pp.\\n59–71. URL: https://ceur-ws.org/Vol-3320/paper7.pdf.\\n\\n\\n'),\n",
       " Document(metadata={'source': '/Users/xitij/Documents/RAG_Projects/Papers/MD/2111.09741v1.md'}, page_content='PATENT SENTIMENT ANALYSIS TO HIGHLIGHT PATENT\\nPARAGRAPHS\\nA DATASET\\nRenukswamy Chikkamath\\nUniversity of Passau\\nPassau, Germany\\nVishvapalsinhji Ramsinh Parmar\\nUniversity of Passau\\nPassau, Germany\\nChristoph Hewel\\nBETTEN & RESCH\\nMunich, Germany\\nMarkus Endres\\nUniversity of Passau\\nPassau, Germany\\nABSTRACT\\nGiven a patent document, identifying distinct semantic annotations is an interesting research aspect.\\nText annotation helps the patent practitioners such as examiners and patent attorneys to quickly\\nidentify the key arguments of any invention, successively providing a timely marking of a patent\\ntext. In the process of manual patent analysis, to attain better readability, recognising the semantic\\ninformation by marking paragraphs is in practice. This semantic annotation process is laborious and\\ntime-consuming. To alleviate such a problem, we proposed a novel dataset to train Machine Learning\\nalgorithms to automate the highlighting process. The contributions of this work are: i) we developed\\na multi-class, novel dataset of size 150k samples by traversing USPTO patents over a decade, ii)\\narticulated statistics and distributions of data using imperative exploratory data analysis, iii) baseline\\nMachine Learning models are developed to utilize the dataset to address patent paragraph highlighting\\ntask, iv) dataset and codes relating to this task are open-sourced through a dedicated GIT web page:\\nhttps://github.com/Renuk9390/Patent_Sentiment_Analysis, and v) future path to extend\\nthis work using Deep Learning and domain speciﬁc pre-trained language models to develop a tool\\nto highlight is provided. This work assist patent practitioners in highlighting semantic information\\nautomatically and aid to create a sustainable and efﬁcient patent analysis using the aptitude of Machine\\nLearning.\\nKeywords Patents · Patent sentiment analysis · Machine learning · Patent information retrieval\\n1\\nIntroduction\\nPatents are the authority awarded monopolies, granted for innovations that are novel, inventive, and non-obvious in\\nnature. Any individual who wishes to get a grant for an idea must draft a detailed technical patent application document.\\nFurther application documents are ﬁled at patent ofﬁces, undergo an extensive examination process at respective patent\\nofﬁces. This is where the role of an examiner comes into the picture. Oftentimes, examiners need a careful reading\\nof patent applications in order to ﬁnd the relevant prior art to the technical subject ﬁeld. The prior art searching can\\nbe done in patent applications, other patent databases, and any non-patent literature. Oftentimes because of indexing\\nin databases, a quick listing of documents can be done. Although there is a list of prior art documents, this is still a\\nlaborious and time-consuming process for examiners to mark relevant technical subject matters in time and make a\\ndecision on the inventive step of the application. Marking or highlighting such subject matters are crucial activities,\\nwhich are manually done in practice. Oftentimes patent examiners and attorneys highlight the text passages which could\\nbe essential entities for key arguments when patent documents are compared to the existing literature. The important\\nentities such as “technical advantages” of any invention, “problems” associated with previous efforts, or other plain\\ndescriptive texts aka “boilerplate” text are manually highlighted by attorneys to compare and contrast individual subject\\nmatters. The highlighted text not only helps the examiner to write a detailed search report but also aid in formal hearings\\narXiv:2111.09741v1  [cs.LG]  6 Nov 2021\\n\\n\\nHIGHLIGHT PATENT PARAGRAPHS\\nto overcome if any objections from patent applicants. With the help of trained machine, text in the paragraph can be\\nhighlighted as shown in Figure 1.\\nFigure 1: Highlighting important text respect to appropriate title\\nOn the other hand, patent applicants or their representatives such as attorneys also read various patent documents\\nin support of their client. In such cases, there is a necessity to keep track of various advantages of the invention in\\ncomparison to the prior art. Because this deﬁnes the scope of the claims, as the scope of claims becomes broader then\\nthe examination becomes more complex and critical. It may end up with several iterations of amendments1 as per the\\nreports from examiners. Patent attorneys also make use of such manual marking of paragraphs of relevant semantic\\ntext. This gives us a shred of evidence that patent paragraph highlighting is one of the important sub-task in patent\\nanalysis. The rest of the paper is organized as follows: Section 2 provides motivation to perform this task and also its\\nscope towards the patent community. Section 3 articulates other related work in this direction to highlight text. Section\\n4 discloses the method to generate a dataset and also a detailed exploratory analysis on the same. Section 5 describes\\nthe models and their performances in highlighting aka patent sentiment analysis process. Section 6 concludes the paper\\nwith future works where a convergence of Deep Learning (DL) models to this end are exposed.\\n2\\nMotivation and Scope\\nPatent application drafting varies around the globe in accordance with region, and also speciﬁc to literature styles.\\nFor instance, the innovations or applications drafted from the Asian region often specify the key matters of the\\ninvention in speciﬁc headers or sections in a patent. These technical matters are often effective technical advantageous\\nof the invention, in other cases, they try to mention selling points of the invention with a separate heading such\\nas “Advantageous effects of invention (AEI)”. Similarly, there are various other annotations that are possible in\\ndifferentiating technical subject matters such as “Technical problems (TP)” associated with previous efforts or any other\\nboilerplate (plain descriptive text, solutions to problem (SP)). Such annotations or dedicated sections help to wade\\nthrough the patents and also increase the readability by keeping track of pieces of evidence. However, such pre-deﬁned\\nannotations in patents are not common in all patent documents.\\nIn recent years, the ﬂooding effect2 of patent applications has greatly increased the workload at examination ofﬁces to\\nprosecute inventions. To effectively engage in such prosecution, the examiner has to perform a wide range of activities\\nsuch as a search for prior art, evaluation of inventiveness within the boundaries of patent law, and also to provide a\\ncritical assessment of decision in a form report or hearing. In the recent past, few approaches Chikkamath et al. [2020],\\nKrestel et al. [2021] shown interest in aligning patent analysis strategies using the scope of DL methodologies to address\\ncomplex patent processes. In such a complex process, simple highlighting automation can ease the documentation and\\nmark of relevant paragraphs for judgment for discerning the non-obviousness of inventions. To this end, we propose a\\ndataset to train a patent paragraph highlighting models, by which patent practitioners work on patent documents in a\\nmuch-assisted way and keep track of their evidence. To the best of our knowledge, there is no evidence in the literature\\nthat focuses on dataset and training models for patent paragraphs highlighting based on Machine Learning (ML). The\\ndataset can be used to train algorithms on both sentence-level and paragraph levels.\\n3\\nRelated Work\\nPatent annotation is a form of information retrieval, conducted in a variety of settings in literature such as keywords or\\nrule-based mining Li et al. [2015], Rodriguez et al. [2015], Iwayama et al. [2003], supervised learning Nanba et al.\\n1https://www.epo.org/law-practice/legal-texts/guidelines.html\\n2https://www.wipo.int/edocs/pubdocs/en/wipo_pub_941_2020.pdf\\n2\\n\\n\\nHIGHLIGHT PATENT PARAGRAPHS\\n[2008], Guangpu et al. [2011] to extract data, and other semi-supervised approaches Brin [1998], Agichtein and Gravano\\n[2000] too. However, it is observed that there are no interests shown in the recent past towards highlighting patent\\nparagraphs using ML. Chen Chen and Deng [2015] proposed to use annotation on retrieving effect classes from patent\\nabstract, and further extend the dataset just with few labeled samples. There are associated drawbacks in state-of-the-art\\nsuch as manual moderations to keyword extraction, semi-automated methods to increase the supervised data generation\\nwith labels for training. Unlike our method, the literature mentioned above is purely based on information extraction\\nbased on syntax and the semantic nature of the text. However, we propose to use the contextual nature of the text,\\nboth on sentence-level and paragraph-level to identify the important subject matters in patent text. In addition, other\\npre-existing markup-based and rule-based approaches proposed to annotate patent documents Agatonovic et al. [2008].\\nThis helped to an extent in identifying metadata of patent. However, such rule-based and markup-based methods face\\ndifﬁculties in identifying the arguable subject matters and contextual features of patents.\\nOther patent literature based annotations include semantic annotations based on ontology Wang et al. [2014]. Such\\nan Ontology-based approach again depends on a manually built initial set of patterns. Ontology-based methods pose\\nsub-optimal results in identifying contextual features. Some of the paid solutions such as PATSEER3 and Patsnap4\\nproposed to highlight patent text automatically, but this is only possible when users know the initial set of keywords and\\nsolutions are not open-sourced. As the name indicate, they focus only on highlighting keywords, however in patent\\nliterature vocabulary is very rich and diverse. Oftentimes, patent applicants come up with new terminologies, therefore\\nkeyword-based approaches often focused less on atomizing patent analysis tasks. A recent interest showed by private\\nIP professionals to use Artiﬁcial Intelligence in highlighting patent text by IPGoggles5, however, the AI algorithms\\nproposed to use are trained on general English literature, and also no evidence of whether the tools will be made public.\\nTo the best of our knowledge, there are no open-sourced datasets and trained models for patent paragraphs highlighting\\nas an annotation process. We propose a novel dataset to highlight patent paragraphs automatically, again this is a\\nmulti-class labeled dataset where a pertinent evidence for the correctness of gold standard labels is given in Section 4.\\n4\\nDataset\\n4.1\\nData Collection\\nPatent sentiment analysis dataset is a curated, selectively extracted collection from the United States Patent and\\nTrademark Ofﬁce (USPTO) raw XML ﬁles. USPTO, to drive advances in innovations and creativity, provides patent\\ngrants6 with full text in nested XML formatted ﬁles to the public. For every year, there are grants published are stacked\\nweekly in zipped ﬁles (for eg: ipg200107.zip, ﬁrst week of January 2020). For instance, the link7 contains 52 XML\\nﬁles, which are arranged according to every week of the year 2020. Each of 52 XML ﬁles is again nested structures,\\ncontains all published grants for that particular week. We parsed XML ﬁles and collected data in a CSV ﬁle, the general\\nworkﬂow of this work is shown in Figure 2.\\nFigure 2: XML ﬁle parsing and modelling\\nBased on our investigation in Google patents8 and interviews with a domain expert9, we found that patent drafting\\npractices and form varies across the globe. For instance, the patent applications originating from Asian countries\\n3https://patseer.com/2021/02/accelerate-your-patent-reviewing-with-patseers-multi-color-highlighting/\\n4https://help.patsnap.com/hc/en-us/articles/115005478629-What-Can-I-Do-When-I-View-A-Patent-\\n5https://ipgoggles.com/\\n6https://developer.uspto.gov/product/patent-grant-full-text-dataxml\\n7https://bulkdata.uspto.gov/data/patent/grant/redbook/fulltext/2020/\\n8A Japan based patent with explicitly mentioned tags (AEI, TP, and SP): https://patents.google.com/patent/\\nUS10834907B2/en?oq=US10834907B2\\n9https://www.bettenpat.com/en-team-hewel.html\\n3\\n\\n\\nHIGHLIGHT PATENT PARAGRAPHS\\npreferred to explicitly mention several selling points of their invention and drawbacks associated with their referred\\nsources in order to increase the chances of patenting. Such clearly deﬁned structures as dedicated sections in patents\\nhelp the examiners and attorneys to wade through patent quickly in order to identify key highlights and arguable subject\\nmatters. Therefore, we decided to look for such special tags which could be crucial entities during the examination\\nand comparing any existing literature. As shown in Figure 2, three possible tags are selected, however, these tags are\\nuncommon in all patents. In other words, oftentimes availability of such tags is scattered, for example, for the year\\n2020, out of nearly 200000 grants only around 8000 patents were found with such explicit text segments. So, it is\\nnecessary to generate a dataset with special text segments and train algorithms further to automatically highlight the\\ntext in any patents where there are no special tags to ease patent analysis.\\nAs shown in Figure 3, “interested data” are considered as three different classes in our dataset. The text paragraphs\\nfollowing the mentioned tags are the text segments (a list of paragraphs) for training any ML algorithms. Three classes\\nnamely positive, negative, and neutral vary with their samples every year, however, to have the class balance we retain\\nonly an equal number of samples in each class. Along with text, other information such as title, and publication number\\nis collected and made available in the dataset. A sample format of the data with respective labels is shown in Table 1.\\nTable 1: Sample dataset\\nDoc Num\\nTitle\\nText\\nLabel\\nUS10842211B2\\nHeat-retaining article\\nWith the clothing..\\n2\\nUS10757923B2\\nAquaculture system\\nAn embodiment of..\\n0\\nUS9855011B2\\nMeasurement device\\nAccording to the..\\n1\\n...\\n...\\n...\\n...\\nFigure 3: Label attachments according to text descriptions\\n4.2\\nExploratory Data Analysis\\nThe XML parsers we developed in this work identiﬁes the interested tags (AIE, TP, and SP) and retrieve the paragraphs\\nrelating to particular tags. The text content is organised in various paragraphs, each paragraph is identiﬁed with unique\\nnumber in the XML document, for instance <p id=\"p-0021\" num=\"0020”> as shown in Figure 4, similarly different\\nparagraphs are collected in a list. The number of paragraphs and words that occur in each paragraph often varies and\\nthis again depends on the applicant’s drafting style and scope of the invention. The total number of paragraphs in\\npositive samples ranges from 0-41, for negative 0-25, and for neutral samples 0-140, some of the paragraphs count for\\neach category is mentioned in Table 2. Number of paragraphs (# Par) and total number available in dataset (Count) are\\nrecorded for each class. For instance, maximum number of paragraphs available in “Neg” class is 25 and respective\\ntotal count in the dataset is 2. Similarly for other classess, where null value indicated non-availablities of data, for\\ninstance maximum paragraphs for “Pos” class is 41 however, there is null value for tuple with 25, means to say there\\nare no patents having positive samples with a size of 25 paragraphs in the dataset.\\nThe number of words that appear in each paragraph must be accountable because this deﬁnes the way how we train\\nalgorithms with respect to sequence lengths. On the other hand, it is interesting to investigate which paragraphs\\ncontribute more towards the type of labels. In other words, applicants can specify advantageous effects in the beginning\\nparagraphs, in middle paragraphs, or else maybe towards the end paragraphs in a list of paragraphs under one positive\\n4\\n\\n\\nHIGHLIGHT PATENT PARAGRAPHS\\nFigure 4: A sample XML patent ﬁle\\nTable 2: Total number of paragraphs\\n(# Par)\\n(Count)\\nPos\\nNeg\\nNeu\\n0\\n27\\n73\\n75\\n1\\n39562\\n23194\\n10401\\n..\\n..\\n..\\n..\\n25\\n(max par for Neg)\\nnull\\n2\\n6\\n41\\n(max par for Pos)\\n3\\nnull\\nnull\\n140\\n(max par for Neu)\\nnull\\nnull\\n7\\ntag. This applies to all tags and their relevant contents. Therefore, training algorithms both at the passage level and\\nsentence level are preferred. The example distributions of words over different paragraphs in the dataset are shown in\\nFigure 5.\\nBased on the investigation made on the dataset using data analysis, we found various issues associated such as i) null\\nvalues under the tags because of format issues in XML ﬁles, ii) various duplicates, where the same content appearing in\\npatent grants with different document numbers, and iii) data samples where word counts are less than 10 approximately.\\n4.2.1\\nPreprocessing\\nIn order to mitigate the above issues, we adopted various pre-processing techniques. There are possible cases in patent\\ndrafting, such as immediately below the special tags instead of text paragraphs, there might be sub-headings that are\\nnot considered by our parsers. The usage of sub-headings, other sections or any images, etc under the special tags is\\ndynamic and poses challenges in writing a universal parser. Furthermore, there are possible cases where authors quickly\\nmake references under special tags, this results in samples where word count less than 10 and also 0 paragraphs because\\nof empty xml tags. Such samples with less count and other with null values are eliminated.\\nAnother main issue is duplicate values, for instance, USPTO list both US10516895B2 and US9866861B2 with the\\nsame title, mostly similar abstracts, also the data under special tags are completely the same. Such samples are also\\neliminated to avoid duplicates in the dataset. The total number of duplicates in each class of dataset is mentioned in\\n5\\n\\n\\nHIGHLIGHT PATENT PARAGRAPHS\\nFigure 5: Relation between word counts and paragraph for negative text\\nTable 3. In addition, the removal of stopwords, special tags, numbers, and other non-text matters is removed from\\nthe dataset. Keras-based10 tokenization is employed to faster the execution of tokenization, where 60 seconds time is\\naccounted for the same using Google Colab11. Special punctuations and capitals are avoided in order to decrease the\\nunique words space. More detailed analysis and other exploratory experiments are available in the link12.\\n4.2.2\\nStatistics\\nThe total numbers of samples in dataset is around 150k (when balanced class-wise), where the distributions in each\\nclass along with duplicates are given in Table 3. The average sequence length of samples ranges from 120 to 593\\nincluding all classes, on the other hand, maximum sequence length ranges from nearly 4000 to 7000 words. More about\\nminimum sequence lengths, standard deviation, and other sequence length distributions are mentioned in Table 6. From\\nthe sequence length, it is clear that descriptions provided in the patents as solutions to problems are often lengthier,\\nwhereas advantageous effects are often shorter. It is also observed from the number of paragraphs in each class that, the\\nmaximum number of patents have their descriptions mentioned in 20 paragraphs as mentioned in Figure 5.\\nTable 3: Size of dataset\\nType\\nCount\\nPos\\n53475\\nBefore pre-process\\nNeg\\n89105\\nNeu\\n63055\\nTotal\\n205635\\nPos\\n48202\\nAfter pre-process\\nNeg\\n79531\\nNeu\\n58043\\nTotal\\n185776\\nPos\\n5242\\nDuplicate values\\nNeg\\n9499\\nNeu\\n4915\\nTotal\\n19656\\nTable 4 provides a clear evidence to depict diminishing explicit subject matters as we travel back in time from 2020 to\\n2010. Identifying technical subject matters for imperative arguments during patent analysis manually adds a greater\\nchallenge. The top tri-grams after stopwords removal found in each class and respective counts are recorded in Table 5.\\nIt is observed that words such as according, invention, present are widely used phrases in the patent literature, although\\n10https://keras.io/\\n11https://colab.research.google.com/notebooks/welcome.ipynb?hl=de\\n12https://github.com/Renuk9390/Patent-annotation/tree/main/\\n6\\n\\n\\nHIGHLIGHT PATENT PARAGRAPHS\\nTable 4: Generated labels from 2010-2020\\nYear\\nTotal grants\\nPositive labels\\nNegative labels\\nNeutral labels\\n2020\\n390572\\n8959\\n15307\\n11026\\n2019\\n392618\\n9131\\n14900\\n10950\\n2018\\n341104\\n7577\\n12080\\n9016\\n2017\\n352547\\n7507\\n11822\\n8794\\n2016\\n334674\\n6893\\n10546\\n7989\\n2015\\n326969\\n6091\\n9232\\n6868\\n2014\\n327014\\n4517\\n7211\\n5100\\n2013\\n303642\\n2132\\n4379\\n2433\\n2012\\n277285\\n535\\n2145\\n663\\n2011\\n248101\\n90\\n971\\n110\\n2010\\n244599\\n11\\n475\\n19\\nthey do not contribute much in distinguishing classes using statistical language processing such as term frequencies and\\ninverse document frequencies (tf-idf), their contextual relationships with other words in paragraphs may contribute.\\nTherefore, we retain those phrases also for training algorithms.\\nTable 5: Top tri-grams in the dataset\\nClass\\nTri-gram\\nCount\\nPos\\n“according present invention”\\n25130\\n“present invention possible”\\n8338\\nNeg\\n“object present invention”\\n52497\\n“present invention provide”\\n50776\\nNeu\\n“aspect present invention”\\n51494\\n“according present invention”\\n34298\\nTable 6: Tokens distributions using Keras\\nType\\n(Class-wise counts)\\nPos\\nNeg\\nNeu\\nmean\\n120.93\\n198.10\\n593.47\\nmin\\n0.00\\n6.00\\n2.00.\\n25%\\n35.00\\n75.00\\n193.00\\n50%\\n61.00\\n138.00\\n410.00\\n75%\\n120.00\\n244.00\\n819.00\\nstd\\n201.55\\n207.96\\n564.33\\nmax\\n3929.00\\n4781.00\\n7317.00\\n5\\nOutcomes\\n5.1\\nBaseline Models\\nA variety of ML models are utilized to test the performance on the dataset, where elemental features include uni-grams,\\nbi-grams, tf-idf scores, and also NBSVM Wang and Manning [2012] architecture-based model. In this setting, classical\\nML models such as Random Forest Classiﬁer (RFC) with maximum estimators of 200 at the maximum depth level\\nof 3, Linear SVC (LSVC), Multinomial Naive Bayes (MNB), and Logistic Regression (LR) with random state 0,\\nmodeled with tf-idf sparse matrix of size (150000, 454182). To reckon the competence of models, we have introduced\\ncross-validation with 5-fold, where it also discourages overﬁtting. The models are trained on 80% of the total data and\\nremaining 20% unseen data is utilized for testing.\\n7\\n\\n\\nHIGHLIGHT PATENT PARAGRAPHS\\nTable 7: Train-Test Sequences\\nType\\nCount\\nUni-grams\\n82412\\nTrain\\nMean-uni\\n306\\nBi-grams\\n1990425\\nMean-bi\\n612\\nMean-uni\\n307\\nTest\\nMean-bi\\n599\\nAnother extended model NBSVM, where core features considered are log-count ratios, technically a combination of\\nvanilla Naive Bayes and Support Vector Machine model. NBSVM speciﬁc sequence lengths on training and test sets\\nare recorded in Table 7.\\n5.2\\nResults\\nThe precision, recall, and F1 scores for NBSVM model are presented in Table 8. Considering individual class the\\nhighest F1 scores achieved by model is 97% for class 2.\\nTable 8: Test scores for NBSVM\\nClass\\nPrecision\\nRecall\\nF1-score\\nSupport\\n0\\n0.96\\n0.96\\n0.96\\n10028\\n1\\n0.95\\n0.96\\n0.95\\n9930\\n2\\n0.97\\n0.96\\n0.97\\n10042\\nThe average scores over 5-fold cross validation for other ML models are recorded in Table 9. We are considering ﬁve\\ndifferent ML models and noting their precision, recall and F1 scores accordingly. Considering these scores the model\\nwhich performs better is the LSVC achieving F1 score of 96%. For the further comparison considering individual fold,\\nbox-plot is presented in Figure 8. It shows that for the RF, its accuracy on the validation dataset varies much more\\nfor the individual folds compared to other three models. To get insight of a model performance the confusion matrix\\nfor the LSVC is shown in Figure 9 for all three labels. The quality of the dataset determines the way how algorithms\\nare trained, and further inﬂuences the performances. The labels considered in this work are reliable, consistent, and\\nground truth evidence is taken from the patent text such that explicitly mentioned tags by applicants are sourced. We\\nanticipate that our attempt helps in identifying subjective information in patent documents automatically. Introducing a\\nnew dataset for patent sentiment analysis and adding a quick usage of this data using ML models are being the focus of\\nthis work, therefore extended experiments and complex DL architectures can be seen in the future work.\\nTo visualize how speciﬁc words are contributing to deciding appropriate labels using the trained model, Python package\\nLime13 is integrated. Figure 6 and 7 shows that how the label 0 and 2 are assigned to the text. It shows top 10 words\\nwith their probability in deciding whether they are in accordance with a label (i.e. 0, 1, 2) or not (i.e. NOT 0, NOT 1,\\nNOT 2). Furthermore, it is highlighted in the text with the speciﬁc word and their respective colour code for better\\ninsight.\\nTable 9: Average 5-fold scores\\nModel\\nPrecision\\nRecall\\nF1-score\\nRFC\\n0.84\\n0.85\\n0.85\\nLSVC\\n0.96\\n0.96\\n0.96\\nMNB\\n0.89\\n0.89\\n0.89\\nLR\\n0.95\\n0.96\\n0.95\\n13https://github.com/marcotcr/lime\\n8\\n\\n\\nHIGHLIGHT PATENT PARAGRAPHS\\nFigure 6: Visualising the LSVC model prediction for the text and deciding neutral label\\nFigure 7: Visualising the LSVC model prediction for the text and deciding negative label\\n6\\nConclusion and Future Work\\nPatent paragraph highlighting is a form of patent sentiment analysis and information retrieval. It is considered as\\na crucial stage in the assessment of patent documents. Patent practitioners such as examiners and attorneys have a\\npractice of aligning their arguments found for technical subject matters of any invention, such alignments are effective\\nthrough bringing automation in highlighting technical entities which are necessary for comparing inventions to existing\\nliterature. To this end, we propose a novel dataset to train ML algorithms to highlight patent paragraphs automatically\\nbased on individual subject matter types. The USPTO full-text grants (for over a decade) in the form of XML ﬁles are\\nconsidered to develop the dataset. Initial baseline models using ML to utilize the dataset and their results are added,\\nwhich helps other researchers to extend work for further reﬁnements. The source codes to collect the raw data, parse\\nthe data to retrieve informative text segments, exploratory data analysis, and model-related codes along with links to\\ndownload dataset are provided to reinforce patent information retrieval.\\nFigure 8: Accuracy scores for different models with 5 fold\\n9\\n\\n\\nHIGHLIGHT PATENT PARAGRAPHS\\nFigure 9: Confusion matrix for LSVC\\nFuture work to this end includes: i) enhance the performances of baseline models using DL with complex learning capa-\\nbilities, ii) introducing domain-speciﬁc pre-trained language models to enforce highlighting tasks, and iii) deployment\\nof extended models as an API to build patent paragraph highlighting tool as a browser extension. Other enhancements\\ntowards the dataset are also possible and interesting future work such as i) extending the dataset for various other\\npossible tags, ii) focusing on other patent ofﬁce grants to make the dataset more universal. We assert that, this work\\nstands as preliminary and opens a new path to bring automation in highlighting tasks in order to ease examination\\nprocess and also to aid other patent practices.\\nReferences\\nRenukswamy Chikkamath, Markus Endres, Lavanya Bayyapu, and Christoph Hewel. An empirical study on patent\\nnovelty detection: A novel approach using machine learning and natural language processing. In 2020 Seventh\\nInternational Conference on Social Networks Analysis, Management and Security (SNAMS), pages 1–7. IEEE, 2020.\\nRalf Krestel, Renukswamy Chikkamath, Christoph Hewel, and Julian Risch. A survey on deep learning for patent\\nanalysis. World Patent Information, 65:102035, 2021.\\nMiao Li, Xinguo Ming, Lina He, Maokuan Zheng, and Zhitao Xu. A triz-based trimming method for patent design\\naround. Computer-Aided Design, 62:20–30, 2015.\\nAndrew Rodriguez, Byunghoon Kim, Jae-Min Lee, Byoung-Yul Coh, and Myong K Jeong. Graph kernel based\\nmeasure for evaluating the inﬂuence of patents in a patent citation network. Expert systems with applications, 42(3):\\n1479–1486, 2015.\\nMakoto Iwayama, Atsushi Fujii, Noriko Kando, and Akihiko Takano. Overview of patent retrieval task at ntcir-3. In\\nProceedings of the ACL-2003 workshop on Patent corpus processing, pages 24–32, 2003.\\nHidetsugu Nanba, Atsushi Fujii, Makoto Iwayama, and Taiichi Hashimoto. Overview of the patent mining task at the\\nntcir-8 workshop. In NTCIR, 2008.\\nFeng Guangpu, Chen Xu, and Peng Zhiyong. A rules and statistical learning based method for chinese patent information\\nextraction. In 2011 Eighth Web Information Systems and Applications Conference, pages 114–118. IEEE, 2011.\\nSergey Brin. Extracting patterns and relations from the world wide web. In International workshop on the world wide\\nweb and databases, pages 172–183. Springer, 1998.\\nEugene Agichtein and Luis Gravano. Snowball: Extracting relations from large plain-text collections. In Proceedings\\nof the ﬁfth ACM conference on Digital libraries, pages 85–94, 2000.\\nXu Chen and Na Deng. A semi-supervised machine learning method for chinese patent effect annotation. In 2015\\ninternational conference on cyber-enabled distributed computing and knowledge discovery, pages 243–250. IEEE,\\n2015.\\nMilan Agatonovic, Niraj Aswani, Kalina Bontcheva, Hamish Cunningham, Thomas Heitz, Yaoyong Li, Ian Roberts,\\nand Valentin Tablan. Large-scale, parallel automatic patent annotation. In Proceedings of the 1st ACM workshop on\\nPatent information retrieval, pages 1–8, 2008.\\nFeng Wang, Lan Fen Lin, and Zhou Yang. An ontology-based automatic semantic annotation approach for patent\\ndocument retrieval in product innovation design. In Applied Mechanics and Materials, volume 446, pages 1581–1590.\\nTrans Tech Publ, 2014.\\nSida I Wang and Christopher D Manning. Baselines and bigrams: Simple, good sentiment and topic classiﬁcation. In\\nProceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),\\npages 90–94, 2012.\\n10\\n\\n\\n')]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6b91a6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,       # max characters per chunk\n",
    "    chunk_overlap=200,      # how much to overlap between chunks\n",
    "    add_start_index=True\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "538d4a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 3 documents into 101 chunks.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "364ca784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "• Numerical and String Data Types: Over 54% of the columns contain numerical data, while 33%\n",
      "are strings. Most tables (49 out of 50) feature at least four columns, except for one single-column\n",
      "table.\n",
      "• Missing Values: Similar to Wikidata, this dataset shows a high number of missing values\n",
      "(39,198), which also lack corresponding annotations in the CEA target file, indicating no need for\n",
      "imputation.\n",
      "• Domain-Specific Patterns: The dataset contains domain-specific characteristics, such as species\n",
      "name abbreviations (e.g., ’C. sclerophylla’ for ’Castanopsis sclerophylla’) and composed values\n",
      "combining multiple elements. Tools like the NCBI Taxonomy database and ChatGPT were used\n",
      "to interpret these domain-specific nuances, ensuring accurate data preprocessing.\n",
      "3.3. tFood Tables\n",
      "The tFood dataset is designed specifically for the Food domain, including two types of tables: Horizontal\n",
      "Relational Tables and Entity Tables. Each table contains two columns, with one column representing\n",
      "{'source': '/Users/xitij/Documents/RAG_Projects/Papers/MD/PD_paper_175.md', 'start_index': 7402}\n"
     ]
    }
   ],
   "source": [
    "document = chunks[10]\n",
    "print(document.page_content)\n",
    "print(document.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a23baefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROMA_PATH = \"/Users/xitij/Documents/RAG_Projects/chroma\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "302cf6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3b534eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(CHROMA_PATH):\n",
    "    shutil.rmtree(CHROMA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7540c594",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0082ee9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(\n",
    "    openai_api_base=\"https://llms-inference.innkube.fim.uni-passau.de\",\n",
    "    api_key=api_key,\n",
    "    model=\"Snowflake/snowflake-arctic-embed-s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7a1e46b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "18def485",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=CHROMA_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b4149469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same embedding model you used while saving\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=CHROMA_PATH,\n",
    "    embedding_function=embeddings  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "af54685c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",  # or \"mmr\" or \"similarity_score_threshold\"\n",
    "    search_kwargs={\"k\": 3}      # get top 3 most relevant chunks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a858bb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c30ebd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(\n",
    "    openai_api_base=\"https://llms-inference.innkube.fim.uni-passau.de\",\n",
    "    api_key=\"sk-Fr2TEolkvrxjjs235KJqkg\",\n",
    "    model=\"qwen2.5\",\n",
    "    temperature=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2511e7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "52d4dd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True  # optional: returns docs along with the answer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "482e7b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "{'query': 'What is patent analysis dataset is about?', 'result': 'The patent analysis dataset is a curated collection of data extracted from the United States Patent and Trademark Office (USPTO) raw XML files. This dataset is designed to support patent sentiment analysis and information retrieval using machine learning techniques. It includes text segments from patent grants, which are parsed from the nested XML structures provided by the USPTO and compiled into a CSV file format. The dataset aims to aid in creating a sustainable and efficient patent analysis process, helping examiners and researchers find relevant prior art more effectively.', 'source_documents': [Document(metadata={'source': '/Users/xitij/Documents/RAG_Projects/Papers/MD/2111.09741v1.md', 'start_index': 10285}, page_content='as an annotation process. We propose a novel dataset to highlight patent paragraphs automatically, again this is a\\nmulti-class labeled dataset where a pertinent evidence for the correctness of gold standard labels is given in Section 4.\\n4\\nDataset\\n4.1\\nData Collection\\nPatent sentiment analysis dataset is a curated, selectively extracted collection from the United States Patent and\\nTrademark Ofﬁce (USPTO) raw XML ﬁles. USPTO, to drive advances in innovations and creativity, provides patent\\ngrants6 with full text in nested XML formatted ﬁles to the public. For every year, there are grants published are stacked\\nweekly in zipped ﬁles (for eg: ipg200107.zip, ﬁrst week of January 2020). For instance, the link7 contains 52 XML\\nﬁles, which are arranged according to every week of the year 2020. Each of 52 XML ﬁles is again nested structures,\\ncontains all published grants for that particular week. We parsed XML ﬁles and collected data in a CSV ﬁle, the general'), Document(metadata={'start_index': 1688, 'source': '/Users/xitij/Documents/RAG_Projects/Papers/MD/2111.09741v1.md'}, page_content='automatically and aid to create a sustainable and efﬁcient patent analysis using the aptitude of Machine\\nLearning.\\nKeywords Patents · Patent sentiment analysis · Machine learning · Patent information retrieval\\n1\\nIntroduction\\nPatents are the authority awarded monopolies, granted for innovations that are novel, inventive, and non-obvious in\\nnature. Any individual who wishes to get a grant for an idea must draft a detailed technical patent application document.\\nFurther application documents are ﬁled at patent ofﬁces, undergo an extensive examination process at respective patent\\nofﬁces. This is where the role of an examiner comes into the picture. Oftentimes, examiners need a careful reading\\nof patent applications in order to ﬁnd the relevant prior art to the technical subject ﬁeld. The prior art searching can\\nbe done in patent applications, other patent databases, and any non-patent literature. Oftentimes because of indexing'), Document(metadata={'source': '/Users/xitij/Documents/RAG_Projects/Papers/MD/2111.09741v1.md', 'start_index': 25465}, page_content='based on individual subject matter types. The USPTO full-text grants (for over a decade) in the form of XML ﬁles are\\nconsidered to develop the dataset. Initial baseline models using ML to utilize the dataset and their results are added,\\nwhich helps other researchers to extend work for further reﬁnements. The source codes to collect the raw data, parse\\nthe data to retrieve informative text segments, exploratory data analysis, and model-related codes along with links to\\ndownload dataset are provided to reinforce patent information retrieval.\\nFigure 8: Accuracy scores for different models with 5 fold\\n9')]}\n"
     ]
    }
   ],
   "source": [
    "query = \"What is patent analysis dataset is about?\"\n",
    "response = qa_chain.invoke(query)\n",
    "\n",
    "print(\"Answer:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2c160a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmexp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
