PATENT SENTIMENT ANALYSIS TO HIGHLIGHT PATENT
PARAGRAPHS
A DATASET
Renukswamy Chikkamath
University of Passau
Passau, Germany
Vishvapalsinhji Ramsinh Parmar
University of Passau
Passau, Germany
Christoph Hewel
BETTEN & RESCH
Munich, Germany
Markus Endres
University of Passau
Passau, Germany
ABSTRACT
Given a patent document, identifying distinct semantic annotations is an interesting research aspect.
Text annotation helps the patent practitioners such as examiners and patent attorneys to quickly
identify the key arguments of any invention, successively providing a timely marking of a patent
text. In the process of manual patent analysis, to attain better readability, recognising the semantic
information by marking paragraphs is in practice. This semantic annotation process is laborious and
time-consuming. To alleviate such a problem, we proposed a novel dataset to train Machine Learning
algorithms to automate the highlighting process. The contributions of this work are: i) we developed
a multi-class, novel dataset of size 150k samples by traversing USPTO patents over a decade, ii)
articulated statistics and distributions of data using imperative exploratory data analysis, iii) baseline
Machine Learning models are developed to utilize the dataset to address patent paragraph highlighting
task, iv) dataset and codes relating to this task are open-sourced through a dedicated GIT web page:
https://github.com/Renuk9390/Patent_Sentiment_Analysis, and v) future path to extend
this work using Deep Learning and domain speciﬁc pre-trained language models to develop a tool
to highlight is provided. This work assist patent practitioners in highlighting semantic information
automatically and aid to create a sustainable and efﬁcient patent analysis using the aptitude of Machine
Learning.
Keywords Patents · Patent sentiment analysis · Machine learning · Patent information retrieval
1
Introduction
Patents are the authority awarded monopolies, granted for innovations that are novel, inventive, and non-obvious in
nature. Any individual who wishes to get a grant for an idea must draft a detailed technical patent application document.
Further application documents are ﬁled at patent ofﬁces, undergo an extensive examination process at respective patent
ofﬁces. This is where the role of an examiner comes into the picture. Oftentimes, examiners need a careful reading
of patent applications in order to ﬁnd the relevant prior art to the technical subject ﬁeld. The prior art searching can
be done in patent applications, other patent databases, and any non-patent literature. Oftentimes because of indexing
in databases, a quick listing of documents can be done. Although there is a list of prior art documents, this is still a
laborious and time-consuming process for examiners to mark relevant technical subject matters in time and make a
decision on the inventive step of the application. Marking or highlighting such subject matters are crucial activities,
which are manually done in practice. Oftentimes patent examiners and attorneys highlight the text passages which could
be essential entities for key arguments when patent documents are compared to the existing literature. The important
entities such as “technical advantages” of any invention, “problems” associated with previous efforts, or other plain
descriptive texts aka “boilerplate” text are manually highlighted by attorneys to compare and contrast individual subject
matters. The highlighted text not only helps the examiner to write a detailed search report but also aid in formal hearings
arXiv:2111.09741v1  [cs.LG]  6 Nov 2021


HIGHLIGHT PATENT PARAGRAPHS
to overcome if any objections from patent applicants. With the help of trained machine, text in the paragraph can be
highlighted as shown in Figure 1.
Figure 1: Highlighting important text respect to appropriate title
On the other hand, patent applicants or their representatives such as attorneys also read various patent documents
in support of their client. In such cases, there is a necessity to keep track of various advantages of the invention in
comparison to the prior art. Because this deﬁnes the scope of the claims, as the scope of claims becomes broader then
the examination becomes more complex and critical. It may end up with several iterations of amendments1 as per the
reports from examiners. Patent attorneys also make use of such manual marking of paragraphs of relevant semantic
text. This gives us a shred of evidence that patent paragraph highlighting is one of the important sub-task in patent
analysis. The rest of the paper is organized as follows: Section 2 provides motivation to perform this task and also its
scope towards the patent community. Section 3 articulates other related work in this direction to highlight text. Section
4 discloses the method to generate a dataset and also a detailed exploratory analysis on the same. Section 5 describes
the models and their performances in highlighting aka patent sentiment analysis process. Section 6 concludes the paper
with future works where a convergence of Deep Learning (DL) models to this end are exposed.
2
Motivation and Scope
Patent application drafting varies around the globe in accordance with region, and also speciﬁc to literature styles.
For instance, the innovations or applications drafted from the Asian region often specify the key matters of the
invention in speciﬁc headers or sections in a patent. These technical matters are often effective technical advantageous
of the invention, in other cases, they try to mention selling points of the invention with a separate heading such
as “Advantageous effects of invention (AEI)”. Similarly, there are various other annotations that are possible in
differentiating technical subject matters such as “Technical problems (TP)” associated with previous efforts or any other
boilerplate (plain descriptive text, solutions to problem (SP)). Such annotations or dedicated sections help to wade
through the patents and also increase the readability by keeping track of pieces of evidence. However, such pre-deﬁned
annotations in patents are not common in all patent documents.
In recent years, the ﬂooding effect2 of patent applications has greatly increased the workload at examination ofﬁces to
prosecute inventions. To effectively engage in such prosecution, the examiner has to perform a wide range of activities
such as a search for prior art, evaluation of inventiveness within the boundaries of patent law, and also to provide a
critical assessment of decision in a form report or hearing. In the recent past, few approaches Chikkamath et al. [2020],
Krestel et al. [2021] shown interest in aligning patent analysis strategies using the scope of DL methodologies to address
complex patent processes. In such a complex process, simple highlighting automation can ease the documentation and
mark of relevant paragraphs for judgment for discerning the non-obviousness of inventions. To this end, we propose a
dataset to train a patent paragraph highlighting models, by which patent practitioners work on patent documents in a
much-assisted way and keep track of their evidence. To the best of our knowledge, there is no evidence in the literature
that focuses on dataset and training models for patent paragraphs highlighting based on Machine Learning (ML). The
dataset can be used to train algorithms on both sentence-level and paragraph levels.
3
Related Work
Patent annotation is a form of information retrieval, conducted in a variety of settings in literature such as keywords or
rule-based mining Li et al. [2015], Rodriguez et al. [2015], Iwayama et al. [2003], supervised learning Nanba et al.
1https://www.epo.org/law-practice/legal-texts/guidelines.html
2https://www.wipo.int/edocs/pubdocs/en/wipo_pub_941_2020.pdf
2


HIGHLIGHT PATENT PARAGRAPHS
[2008], Guangpu et al. [2011] to extract data, and other semi-supervised approaches Brin [1998], Agichtein and Gravano
[2000] too. However, it is observed that there are no interests shown in the recent past towards highlighting patent
paragraphs using ML. Chen Chen and Deng [2015] proposed to use annotation on retrieving effect classes from patent
abstract, and further extend the dataset just with few labeled samples. There are associated drawbacks in state-of-the-art
such as manual moderations to keyword extraction, semi-automated methods to increase the supervised data generation
with labels for training. Unlike our method, the literature mentioned above is purely based on information extraction
based on syntax and the semantic nature of the text. However, we propose to use the contextual nature of the text,
both on sentence-level and paragraph-level to identify the important subject matters in patent text. In addition, other
pre-existing markup-based and rule-based approaches proposed to annotate patent documents Agatonovic et al. [2008].
This helped to an extent in identifying metadata of patent. However, such rule-based and markup-based methods face
difﬁculties in identifying the arguable subject matters and contextual features of patents.
Other patent literature based annotations include semantic annotations based on ontology Wang et al. [2014]. Such
an Ontology-based approach again depends on a manually built initial set of patterns. Ontology-based methods pose
sub-optimal results in identifying contextual features. Some of the paid solutions such as PATSEER3 and Patsnap4
proposed to highlight patent text automatically, but this is only possible when users know the initial set of keywords and
solutions are not open-sourced. As the name indicate, they focus only on highlighting keywords, however in patent
literature vocabulary is very rich and diverse. Oftentimes, patent applicants come up with new terminologies, therefore
keyword-based approaches often focused less on atomizing patent analysis tasks. A recent interest showed by private
IP professionals to use Artiﬁcial Intelligence in highlighting patent text by IPGoggles5, however, the AI algorithms
proposed to use are trained on general English literature, and also no evidence of whether the tools will be made public.
To the best of our knowledge, there are no open-sourced datasets and trained models for patent paragraphs highlighting
as an annotation process. We propose a novel dataset to highlight patent paragraphs automatically, again this is a
multi-class labeled dataset where a pertinent evidence for the correctness of gold standard labels is given in Section 4.
4
Dataset
4.1
Data Collection
Patent sentiment analysis dataset is a curated, selectively extracted collection from the United States Patent and
Trademark Ofﬁce (USPTO) raw XML ﬁles. USPTO, to drive advances in innovations and creativity, provides patent
grants6 with full text in nested XML formatted ﬁles to the public. For every year, there are grants published are stacked
weekly in zipped ﬁles (for eg: ipg200107.zip, ﬁrst week of January 2020). For instance, the link7 contains 52 XML
ﬁles, which are arranged according to every week of the year 2020. Each of 52 XML ﬁles is again nested structures,
contains all published grants for that particular week. We parsed XML ﬁles and collected data in a CSV ﬁle, the general
workﬂow of this work is shown in Figure 2.
Figure 2: XML ﬁle parsing and modelling
Based on our investigation in Google patents8 and interviews with a domain expert9, we found that patent drafting
practices and form varies across the globe. For instance, the patent applications originating from Asian countries
3https://patseer.com/2021/02/accelerate-your-patent-reviewing-with-patseers-multi-color-highlighting/
4https://help.patsnap.com/hc/en-us/articles/115005478629-What-Can-I-Do-When-I-View-A-Patent-
5https://ipgoggles.com/
6https://developer.uspto.gov/product/patent-grant-full-text-dataxml
7https://bulkdata.uspto.gov/data/patent/grant/redbook/fulltext/2020/
8A Japan based patent with explicitly mentioned tags (AEI, TP, and SP): https://patents.google.com/patent/
US10834907B2/en?oq=US10834907B2
9https://www.bettenpat.com/en-team-hewel.html
3


HIGHLIGHT PATENT PARAGRAPHS
preferred to explicitly mention several selling points of their invention and drawbacks associated with their referred
sources in order to increase the chances of patenting. Such clearly deﬁned structures as dedicated sections in patents
help the examiners and attorneys to wade through patent quickly in order to identify key highlights and arguable subject
matters. Therefore, we decided to look for such special tags which could be crucial entities during the examination
and comparing any existing literature. As shown in Figure 2, three possible tags are selected, however, these tags are
uncommon in all patents. In other words, oftentimes availability of such tags is scattered, for example, for the year
2020, out of nearly 200000 grants only around 8000 patents were found with such explicit text segments. So, it is
necessary to generate a dataset with special text segments and train algorithms further to automatically highlight the
text in any patents where there are no special tags to ease patent analysis.
As shown in Figure 3, “interested data” are considered as three different classes in our dataset. The text paragraphs
following the mentioned tags are the text segments (a list of paragraphs) for training any ML algorithms. Three classes
namely positive, negative, and neutral vary with their samples every year, however, to have the class balance we retain
only an equal number of samples in each class. Along with text, other information such as title, and publication number
is collected and made available in the dataset. A sample format of the data with respective labels is shown in Table 1.
Table 1: Sample dataset
Doc Num
Title
Text
Label
US10842211B2
Heat-retaining article
With the clothing..
2
US10757923B2
Aquaculture system
An embodiment of..
0
US9855011B2
Measurement device
According to the..
1
...
...
...
...
Figure 3: Label attachments according to text descriptions
4.2
Exploratory Data Analysis
The XML parsers we developed in this work identiﬁes the interested tags (AIE, TP, and SP) and retrieve the paragraphs
relating to particular tags. The text content is organised in various paragraphs, each paragraph is identiﬁed with unique
number in the XML document, for instance <p id="p-0021" num="0020”> as shown in Figure 4, similarly different
paragraphs are collected in a list. The number of paragraphs and words that occur in each paragraph often varies and
this again depends on the applicant’s drafting style and scope of the invention. The total number of paragraphs in
positive samples ranges from 0-41, for negative 0-25, and for neutral samples 0-140, some of the paragraphs count for
each category is mentioned in Table 2. Number of paragraphs (# Par) and total number available in dataset (Count) are
recorded for each class. For instance, maximum number of paragraphs available in “Neg” class is 25 and respective
total count in the dataset is 2. Similarly for other classess, where null value indicated non-availablities of data, for
instance maximum paragraphs for “Pos” class is 41 however, there is null value for tuple with 25, means to say there
are no patents having positive samples with a size of 25 paragraphs in the dataset.
The number of words that appear in each paragraph must be accountable because this deﬁnes the way how we train
algorithms with respect to sequence lengths. On the other hand, it is interesting to investigate which paragraphs
contribute more towards the type of labels. In other words, applicants can specify advantageous effects in the beginning
paragraphs, in middle paragraphs, or else maybe towards the end paragraphs in a list of paragraphs under one positive
4


HIGHLIGHT PATENT PARAGRAPHS
Figure 4: A sample XML patent ﬁle
Table 2: Total number of paragraphs
(# Par)
(Count)
Pos
Neg
Neu
0
27
73
75
1
39562
23194
10401
..
..
..
..
25
(max par for Neg)
null
2
6
41
(max par for Pos)
3
null
null
140
(max par for Neu)
null
null
7
tag. This applies to all tags and their relevant contents. Therefore, training algorithms both at the passage level and
sentence level are preferred. The example distributions of words over different paragraphs in the dataset are shown in
Figure 5.
Based on the investigation made on the dataset using data analysis, we found various issues associated such as i) null
values under the tags because of format issues in XML ﬁles, ii) various duplicates, where the same content appearing in
patent grants with different document numbers, and iii) data samples where word counts are less than 10 approximately.
4.2.1
Preprocessing
In order to mitigate the above issues, we adopted various pre-processing techniques. There are possible cases in patent
drafting, such as immediately below the special tags instead of text paragraphs, there might be sub-headings that are
not considered by our parsers. The usage of sub-headings, other sections or any images, etc under the special tags is
dynamic and poses challenges in writing a universal parser. Furthermore, there are possible cases where authors quickly
make references under special tags, this results in samples where word count less than 10 and also 0 paragraphs because
of empty xml tags. Such samples with less count and other with null values are eliminated.
Another main issue is duplicate values, for instance, USPTO list both US10516895B2 and US9866861B2 with the
same title, mostly similar abstracts, also the data under special tags are completely the same. Such samples are also
eliminated to avoid duplicates in the dataset. The total number of duplicates in each class of dataset is mentioned in
5


HIGHLIGHT PATENT PARAGRAPHS
Figure 5: Relation between word counts and paragraph for negative text
Table 3. In addition, the removal of stopwords, special tags, numbers, and other non-text matters is removed from
the dataset. Keras-based10 tokenization is employed to faster the execution of tokenization, where 60 seconds time is
accounted for the same using Google Colab11. Special punctuations and capitals are avoided in order to decrease the
unique words space. More detailed analysis and other exploratory experiments are available in the link12.
4.2.2
Statistics
The total numbers of samples in dataset is around 150k (when balanced class-wise), where the distributions in each
class along with duplicates are given in Table 3. The average sequence length of samples ranges from 120 to 593
including all classes, on the other hand, maximum sequence length ranges from nearly 4000 to 7000 words. More about
minimum sequence lengths, standard deviation, and other sequence length distributions are mentioned in Table 6. From
the sequence length, it is clear that descriptions provided in the patents as solutions to problems are often lengthier,
whereas advantageous effects are often shorter. It is also observed from the number of paragraphs in each class that, the
maximum number of patents have their descriptions mentioned in 20 paragraphs as mentioned in Figure 5.
Table 3: Size of dataset
Type
Count
Pos
53475
Before pre-process
Neg
89105
Neu
63055
Total
205635
Pos
48202
After pre-process
Neg
79531
Neu
58043
Total
185776
Pos
5242
Duplicate values
Neg
9499
Neu
4915
Total
19656
Table 4 provides a clear evidence to depict diminishing explicit subject matters as we travel back in time from 2020 to
2010. Identifying technical subject matters for imperative arguments during patent analysis manually adds a greater
challenge. The top tri-grams after stopwords removal found in each class and respective counts are recorded in Table 5.
It is observed that words such as according, invention, present are widely used phrases in the patent literature, although
10https://keras.io/
11https://colab.research.google.com/notebooks/welcome.ipynb?hl=de
12https://github.com/Renuk9390/Patent-annotation/tree/main/
6


HIGHLIGHT PATENT PARAGRAPHS
Table 4: Generated labels from 2010-2020
Year
Total grants
Positive labels
Negative labels
Neutral labels
2020
390572
8959
15307
11026
2019
392618
9131
14900
10950
2018
341104
7577
12080
9016
2017
352547
7507
11822
8794
2016
334674
6893
10546
7989
2015
326969
6091
9232
6868
2014
327014
4517
7211
5100
2013
303642
2132
4379
2433
2012
277285
535
2145
663
2011
248101
90
971
110
2010
244599
11
475
19
they do not contribute much in distinguishing classes using statistical language processing such as term frequencies and
inverse document frequencies (tf-idf), their contextual relationships with other words in paragraphs may contribute.
Therefore, we retain those phrases also for training algorithms.
Table 5: Top tri-grams in the dataset
Class
Tri-gram
Count
Pos
“according present invention”
25130
“present invention possible”
8338
Neg
“object present invention”
52497
“present invention provide”
50776
Neu
“aspect present invention”
51494
“according present invention”
34298
Table 6: Tokens distributions using Keras
Type
(Class-wise counts)
Pos
Neg
Neu
mean
120.93
198.10
593.47
min
0.00
6.00
2.00.
25%
35.00
75.00
193.00
50%
61.00
138.00
410.00
75%
120.00
244.00
819.00
std
201.55
207.96
564.33
max
3929.00
4781.00
7317.00
5
Outcomes
5.1
Baseline Models
A variety of ML models are utilized to test the performance on the dataset, where elemental features include uni-grams,
bi-grams, tf-idf scores, and also NBSVM Wang and Manning [2012] architecture-based model. In this setting, classical
ML models such as Random Forest Classiﬁer (RFC) with maximum estimators of 200 at the maximum depth level
of 3, Linear SVC (LSVC), Multinomial Naive Bayes (MNB), and Logistic Regression (LR) with random state 0,
modeled with tf-idf sparse matrix of size (150000, 454182). To reckon the competence of models, we have introduced
cross-validation with 5-fold, where it also discourages overﬁtting. The models are trained on 80% of the total data and
remaining 20% unseen data is utilized for testing.
7


HIGHLIGHT PATENT PARAGRAPHS
Table 7: Train-Test Sequences
Type
Count
Uni-grams
82412
Train
Mean-uni
306
Bi-grams
1990425
Mean-bi
612
Mean-uni
307
Test
Mean-bi
599
Another extended model NBSVM, where core features considered are log-count ratios, technically a combination of
vanilla Naive Bayes and Support Vector Machine model. NBSVM speciﬁc sequence lengths on training and test sets
are recorded in Table 7.
5.2
Results
The precision, recall, and F1 scores for NBSVM model are presented in Table 8. Considering individual class the
highest F1 scores achieved by model is 97% for class 2.
Table 8: Test scores for NBSVM
Class
Precision
Recall
F1-score
Support
0
0.96
0.96
0.96
10028
1
0.95
0.96
0.95
9930
2
0.97
0.96
0.97
10042
The average scores over 5-fold cross validation for other ML models are recorded in Table 9. We are considering ﬁve
different ML models and noting their precision, recall and F1 scores accordingly. Considering these scores the model
which performs better is the LSVC achieving F1 score of 96%. For the further comparison considering individual fold,
box-plot is presented in Figure 8. It shows that for the RF, its accuracy on the validation dataset varies much more
for the individual folds compared to other three models. To get insight of a model performance the confusion matrix
for the LSVC is shown in Figure 9 for all three labels. The quality of the dataset determines the way how algorithms
are trained, and further inﬂuences the performances. The labels considered in this work are reliable, consistent, and
ground truth evidence is taken from the patent text such that explicitly mentioned tags by applicants are sourced. We
anticipate that our attempt helps in identifying subjective information in patent documents automatically. Introducing a
new dataset for patent sentiment analysis and adding a quick usage of this data using ML models are being the focus of
this work, therefore extended experiments and complex DL architectures can be seen in the future work.
To visualize how speciﬁc words are contributing to deciding appropriate labels using the trained model, Python package
Lime13 is integrated. Figure 6 and 7 shows that how the label 0 and 2 are assigned to the text. It shows top 10 words
with their probability in deciding whether they are in accordance with a label (i.e. 0, 1, 2) or not (i.e. NOT 0, NOT 1,
NOT 2). Furthermore, it is highlighted in the text with the speciﬁc word and their respective colour code for better
insight.
Table 9: Average 5-fold scores
Model
Precision
Recall
F1-score
RFC
0.84
0.85
0.85
LSVC
0.96
0.96
0.96
MNB
0.89
0.89
0.89
LR
0.95
0.96
0.95
13https://github.com/marcotcr/lime
8


HIGHLIGHT PATENT PARAGRAPHS
Figure 6: Visualising the LSVC model prediction for the text and deciding neutral label
Figure 7: Visualising the LSVC model prediction for the text and deciding negative label
6
Conclusion and Future Work
Patent paragraph highlighting is a form of patent sentiment analysis and information retrieval. It is considered as
a crucial stage in the assessment of patent documents. Patent practitioners such as examiners and attorneys have a
practice of aligning their arguments found for technical subject matters of any invention, such alignments are effective
through bringing automation in highlighting technical entities which are necessary for comparing inventions to existing
literature. To this end, we propose a novel dataset to train ML algorithms to highlight patent paragraphs automatically
based on individual subject matter types. The USPTO full-text grants (for over a decade) in the form of XML ﬁles are
considered to develop the dataset. Initial baseline models using ML to utilize the dataset and their results are added,
which helps other researchers to extend work for further reﬁnements. The source codes to collect the raw data, parse
the data to retrieve informative text segments, exploratory data analysis, and model-related codes along with links to
download dataset are provided to reinforce patent information retrieval.
Figure 8: Accuracy scores for different models with 5 fold
9


HIGHLIGHT PATENT PARAGRAPHS
Figure 9: Confusion matrix for LSVC
Future work to this end includes: i) enhance the performances of baseline models using DL with complex learning capa-
bilities, ii) introducing domain-speciﬁc pre-trained language models to enforce highlighting tasks, and iii) deployment
of extended models as an API to build patent paragraph highlighting tool as a browser extension. Other enhancements
towards the dataset are also possible and interesting future work such as i) extending the dataset for various other
possible tags, ii) focusing on other patent ofﬁce grants to make the dataset more universal. We assert that, this work
stands as preliminary and opens a new path to bring automation in highlighting tasks in order to ease examination
process and also to aid other patent practices.
References
Renukswamy Chikkamath, Markus Endres, Lavanya Bayyapu, and Christoph Hewel. An empirical study on patent
novelty detection: A novel approach using machine learning and natural language processing. In 2020 Seventh
International Conference on Social Networks Analysis, Management and Security (SNAMS), pages 1–7. IEEE, 2020.
Ralf Krestel, Renukswamy Chikkamath, Christoph Hewel, and Julian Risch. A survey on deep learning for patent
analysis. World Patent Information, 65:102035, 2021.
Miao Li, Xinguo Ming, Lina He, Maokuan Zheng, and Zhitao Xu. A triz-based trimming method for patent design
around. Computer-Aided Design, 62:20–30, 2015.
Andrew Rodriguez, Byunghoon Kim, Jae-Min Lee, Byoung-Yul Coh, and Myong K Jeong. Graph kernel based
measure for evaluating the inﬂuence of patents in a patent citation network. Expert systems with applications, 42(3):
1479–1486, 2015.
Makoto Iwayama, Atsushi Fujii, Noriko Kando, and Akihiko Takano. Overview of patent retrieval task at ntcir-3. In
Proceedings of the ACL-2003 workshop on Patent corpus processing, pages 24–32, 2003.
Hidetsugu Nanba, Atsushi Fujii, Makoto Iwayama, and Taiichi Hashimoto. Overview of the patent mining task at the
ntcir-8 workshop. In NTCIR, 2008.
Feng Guangpu, Chen Xu, and Peng Zhiyong. A rules and statistical learning based method for chinese patent information
extraction. In 2011 Eighth Web Information Systems and Applications Conference, pages 114–118. IEEE, 2011.
Sergey Brin. Extracting patterns and relations from the world wide web. In International workshop on the world wide
web and databases, pages 172–183. Springer, 1998.
Eugene Agichtein and Luis Gravano. Snowball: Extracting relations from large plain-text collections. In Proceedings
of the ﬁfth ACM conference on Digital libraries, pages 85–94, 2000.
Xu Chen and Na Deng. A semi-supervised machine learning method for chinese patent effect annotation. In 2015
international conference on cyber-enabled distributed computing and knowledge discovery, pages 243–250. IEEE,
2015.
Milan Agatonovic, Niraj Aswani, Kalina Bontcheva, Hamish Cunningham, Thomas Heitz, Yaoyong Li, Ian Roberts,
and Valentin Tablan. Large-scale, parallel automatic patent annotation. In Proceedings of the 1st ACM workshop on
Patent information retrieval, pages 1–8, 2008.
Feng Wang, Lan Fen Lin, and Zhou Yang. An ontology-based automatic semantic annotation approach for patent
document retrieval in product innovation design. In Applied Mechanics and Materials, volume 446, pages 1581–1590.
Trans Tech Publ, 2014.
Sida I Wang and Christopher D Manning. Baselines and bigrams: Simple, good sentiment and topic classiﬁcation. In
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),
pages 90–94, 2012.
10


